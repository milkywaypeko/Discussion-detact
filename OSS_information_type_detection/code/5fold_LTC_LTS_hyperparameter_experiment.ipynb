{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested 5-Fold Cross Validation For Logistic Regression On Textual Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xlrd as xl\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import pprint\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import pickle\n",
    "from operator import itemgetter\n",
    "import time, datetime\n",
    "from functools import partial, update_wrapper\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as Imb_Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report, accuracy_score, make_scorer, confusion_matrix\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "## Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use spaCy parser for word tokenization of a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create an instance of the English parser\n",
    "parser = English()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define stopwords as punctuation + common contractions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = list(punctuation) + [\"'s\",\"'m\",\"n't\",\"'re\",\"-\",\"'ll\",'...'] #+ stopwords.words('english')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code to lemmatize and tokenize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma(item):\n",
    "    return WordNetLemmatizer().lemmatize(item)\n",
    "\n",
    "def tokenize(line):\n",
    "    line_tokens = []\n",
    "    tokens = parser(line)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            line_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            line_tokens.append('SCREEN_NAME')\n",
    "        elif str(token) not in stop_words:\n",
    "            line_tokens.append(get_lemma(token.lower_))\n",
    "    return line_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of corpus: 38350\n"
     ]
    }
   ],
   "source": [
    "### Read from the pickled file\n",
    "all_data = pd.read_csv('../data/combined_data_oversampled.csv')\n",
    "\n",
    "print(\"Size of corpus: \"+str(len(all_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.dropna(subset=['Text Content', 'Code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_remove = [ \"Testing\",'Future Plan','Issue Content Management']\n",
    "all_data = all_data[~all_data['Code'].isin(labels_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 13\n",
      "[   'Action on Issue',\n",
      "    'Bug Reproduction',\n",
      "    'Contribution and Commitment',\n",
      "    'Expected Behaviour',\n",
      "    'Investigation and Exploration',\n",
      "    'Motivation',\n",
      "    'Observed Bug Behaviour',\n",
      "    'Potential New Issues and Requests',\n",
      "    'Social Conversation',\n",
      "    'Solution Discussion',\n",
      "    'Solution Usage',\n",
      "    'Task Progress',\n",
      "    'Workarounds']\n"
     ]
    }
   ],
   "source": [
    "X = all_data['Text Content'].values\n",
    "y = all_data['Code'].values\n",
    "\n",
    "print(\"Number of unique labels: \"+str(len(set(y))))\n",
    "\n",
    "labels = list(set(y))\n",
    "labels.sort()\n",
    "\n",
    "pp.pprint(labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested Cross-Validation on Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be used within GridSearch\n",
    "inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# To be used in outer CV\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Pipeline tfidf + ngram_range + logreg C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline = Imb_Pipeline([\n",
    "    ('vect', TfidfVectorizer(tokenizer=tokenize)),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "### Hyperparameters to search\n",
    "### We can change hyperparameter values here\n",
    "parameters = {\n",
    "    'vect__ngram_range': [(2, 3)],  # unigrams or bigrams\n",
    "    'clf__C': [100],\n",
    "    'vect__max_df': [1],\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested Cross Validation using GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/mintymine/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Pipeline2 scenario in 0:04:46.364975\n"
     ]
    }
   ],
   "source": [
    "### Define and create the scoring functions\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "def score_func(y_true, y_pred, score_index, i):\n",
    "    return(precision_recall_fscore_support(y_true,y_pred)[score_index][i])\n",
    "\n",
    "def avg_score(y_true, y_pred, score_index):\n",
    "    return precision_recall_fscore_support(y_true,y_pred,average='weighted')[score_index]\n",
    "\n",
    "def sum_support(y_true, y_pred):\n",
    "    return len(y_true)\n",
    "\n",
    "### Create partials for each of the metrics returned\n",
    "score_funcs = {v: partial(score_func, score_index=k) for k, v in {0:'precision',1:'recall',2:'fscore',3:'support'}.items()}\n",
    "prec_score = partial(score_func, score_index=0)\n",
    "update_wrapper(prec_score,score_func)\n",
    "rec_score = partial(score_func, score_index=1)\n",
    "update_wrapper(rec_score,score_func)\n",
    "f_score = partial(score_func, score_index=2)\n",
    "update_wrapper(f_score,score_func)\n",
    "support_score = partial(score_func, score_index=3)\n",
    "update_wrapper(support_score,score_func)\n",
    "\n",
    "### Create a callable scoring function for each of the metrics for each classification label\n",
    "scorer = {}\n",
    "for label_id in range(0,13):\n",
    "    scorer['label'+str(label_id)+'_precision'] = make_scorer(prec_score, i=label_id)\n",
    "    scorer['label'+str(label_id)+'_recall'] = make_scorer(rec_score, i=label_id)\n",
    "    scorer['label'+str(label_id)+'_fscore'] = make_scorer(f_score, i=label_id)\n",
    "    scorer['label'+str(label_id)+'_support'] = make_scorer(support_score, i=label_id)\n",
    "\n",
    "### Create a callable scoring function for avg/total of the metrics across classification labels\n",
    "scorer['avg_precision'] = make_scorer(avg_score,score_index=0)\n",
    "scorer['avg_recall'] = make_scorer(avg_score,score_index=1)\n",
    "scorer['avg_fscore'] = make_scorer(avg_score,score_index=2)\n",
    "scorer['total_support'] = make_scorer(sum_support)\n",
    "\n",
    "\n",
    "### Perform Nested cross-validation on Pipeline\n",
    "start = time.time()\n",
    "clf = GridSearchCV(pipeline, parameters, cv=inner_cv, scoring='f1_weighted')\n",
    "clf_results = cross_validate(clf, X=X, y=y, cv=outer_cv, scoring=scorer,error_score='raise')\n",
    "print(\"Completed Pipeline2 scenario in \"+ str(datetime.timedelta(seconds=(time.time()-start))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display and Save Training and Testing Results for each Fold:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------- FOLD 0: -------------------------\n",
      "\n",
      "Training Results:\n",
      "Empty DataFrame\n",
      "Columns: [Precision, Recall, F1-score, Support]\n",
      "Index: []\n",
      "\n",
      "Test Results:\n",
      "                                   Precision  Recall  F1-score  Support\n",
      "Action on Issue                         0.08    1.00      0.14    590.0\n",
      "Bug Reproduction                        0.00    0.00      0.00    590.0\n",
      "Contribution and Commitment             0.00    0.00      0.00    590.0\n",
      "Expected Behaviour                      0.00    0.00      0.00    590.0\n",
      "Investigation and Exploration           0.00    0.00      0.00    590.0\n",
      "Motivation                              0.00    0.00      0.00    590.0\n",
      "Observed Bug Behaviour                  0.00    0.00      0.00    590.0\n",
      "Potential New Issues and Requests       0.00    0.00      0.00    590.0\n",
      "Social Conversation                     0.00    0.00      0.00    590.0\n",
      "Solution Discussion                     0.00    0.00      0.00    590.0\n",
      "Solution Usage                          0.00    0.00      0.00    590.0\n",
      "Task Progress                           0.00    0.00      0.00    590.0\n",
      "Workarounds                             0.00    0.00      0.00    590.0\n",
      "Avg/Total                               0.01    0.08      0.01   7670.0\n",
      "\n",
      "------------------------- FOLD 1: -------------------------\n",
      "\n",
      "Training Results:\n",
      "Empty DataFrame\n",
      "Columns: [Precision, Recall, F1-score, Support]\n",
      "Index: []\n",
      "\n",
      "Test Results:\n",
      "                                   Precision  Recall  F1-score  Support\n",
      "Action on Issue                         0.08    1.00      0.14    590.0\n",
      "Bug Reproduction                        0.00    0.00      0.00    590.0\n",
      "Contribution and Commitment             0.00    0.00      0.00    590.0\n",
      "Expected Behaviour                      0.00    0.00      0.00    590.0\n",
      "Investigation and Exploration           0.00    0.00      0.00    590.0\n",
      "Motivation                              0.00    0.00      0.00    590.0\n",
      "Observed Bug Behaviour                  0.00    0.00      0.00    590.0\n",
      "Potential New Issues and Requests       0.00    0.00      0.00    590.0\n",
      "Social Conversation                     0.00    0.00      0.00    590.0\n",
      "Solution Discussion                     0.00    0.00      0.00    590.0\n",
      "Solution Usage                          0.00    0.00      0.00    590.0\n",
      "Task Progress                           0.00    0.00      0.00    590.0\n",
      "Workarounds                             0.00    0.00      0.00    590.0\n",
      "Avg/Total                               0.01    0.08      0.01   7670.0\n",
      "\n",
      "------------------------- FOLD 2: -------------------------\n",
      "\n",
      "Training Results:\n",
      "Empty DataFrame\n",
      "Columns: [Precision, Recall, F1-score, Support]\n",
      "Index: []\n",
      "\n",
      "Test Results:\n",
      "                                   Precision  Recall  F1-score  Support\n",
      "Action on Issue                         0.08    1.00      0.14    590.0\n",
      "Bug Reproduction                        0.00    0.00      0.00    590.0\n",
      "Contribution and Commitment             0.00    0.00      0.00    590.0\n",
      "Expected Behaviour                      0.00    0.00      0.00    590.0\n",
      "Investigation and Exploration           0.00    0.00      0.00    590.0\n",
      "Motivation                              0.00    0.00      0.00    590.0\n",
      "Observed Bug Behaviour                  0.00    0.00      0.00    590.0\n",
      "Potential New Issues and Requests       0.00    0.00      0.00    590.0\n",
      "Social Conversation                     0.00    0.00      0.00    590.0\n",
      "Solution Discussion                     0.00    0.00      0.00    590.0\n",
      "Solution Usage                          0.00    0.00      0.00    590.0\n",
      "Task Progress                           0.00    0.00      0.00    590.0\n",
      "Workarounds                             0.00    0.00      0.00    590.0\n",
      "Avg/Total                               0.01    0.08      0.01   7670.0\n",
      "\n",
      "------------------------- FOLD 3: -------------------------\n",
      "\n",
      "Training Results:\n",
      "Empty DataFrame\n",
      "Columns: [Precision, Recall, F1-score, Support]\n",
      "Index: []\n",
      "\n",
      "Test Results:\n",
      "                                   Precision  Recall  F1-score  Support\n",
      "Action on Issue                         0.08    1.00      0.14    590.0\n",
      "Bug Reproduction                        0.00    0.00      0.00    590.0\n",
      "Contribution and Commitment             0.00    0.00      0.00    590.0\n",
      "Expected Behaviour                      0.00    0.00      0.00    590.0\n",
      "Investigation and Exploration           0.00    0.00      0.00    590.0\n",
      "Motivation                              0.00    0.00      0.00    590.0\n",
      "Observed Bug Behaviour                  0.00    0.00      0.00    590.0\n",
      "Potential New Issues and Requests       0.00    0.00      0.00    590.0\n",
      "Social Conversation                     0.00    0.00      0.00    590.0\n",
      "Solution Discussion                     0.00    0.00      0.00    590.0\n",
      "Solution Usage                          0.00    0.00      0.00    590.0\n",
      "Task Progress                           0.00    0.00      0.00    590.0\n",
      "Workarounds                             0.00    0.00      0.00    590.0\n",
      "Avg/Total                               0.01    0.08      0.01   7670.0\n",
      "\n",
      "------------------------- FOLD 4: -------------------------\n",
      "\n",
      "Training Results:\n",
      "Empty DataFrame\n",
      "Columns: [Precision, Recall, F1-score, Support]\n",
      "Index: []\n",
      "\n",
      "Test Results:\n",
      "                                   Precision  Recall  F1-score  Support\n",
      "Action on Issue                         0.08    1.00      0.14    590.0\n",
      "Bug Reproduction                        0.00    0.00      0.00    590.0\n",
      "Contribution and Commitment             0.00    0.00      0.00    590.0\n",
      "Expected Behaviour                      0.00    0.00      0.00    590.0\n",
      "Investigation and Exploration           0.00    0.00      0.00    590.0\n",
      "Motivation                              0.00    0.00      0.00    590.0\n",
      "Observed Bug Behaviour                  0.00    0.00      0.00    590.0\n",
      "Potential New Issues and Requests       0.00    0.00      0.00    590.0\n",
      "Social Conversation                     0.00    0.00      0.00    590.0\n",
      "Solution Discussion                     0.00    0.00      0.00    590.0\n",
      "Solution Usage                          0.00    0.00      0.00    590.0\n",
      "Task Progress                           0.00    0.00      0.00    590.0\n",
      "Workarounds                             0.00    0.00      0.00    590.0\n",
      "Avg/Total                               0.01    0.08      0.01   7670.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_report = pd.DataFrame(columns=['Precision', 'Recall', 'F1-score', 'Support'])\n",
    "test_report = pd.DataFrame(columns=['Precision', 'Recall', 'F1-score', 'Support'])\n",
    "\n",
    "result_dict = {}\n",
    "\n",
    "writer = pd.ExcelWriter('../results/hyperparameter_result5.xlsx')\n",
    "\n",
    "\n",
    "datalength = 0\n",
    "\n",
    "for i in range(0, 5):\n",
    "    for label_id in range(0, 13):\n",
    "        train_label_precision_key = 'train_label' + str(label_id) + '_precision'\n",
    "        train_label_recall_key = 'train_label' + str(label_id) + '_recall'\n",
    "        train_label_fscore_key = 'train_label' + str(label_id) + '_fscore'\n",
    "        train_label_support_key = 'train_label' + str(label_id) + '_support'\n",
    "\n",
    "        if train_label_precision_key in clf_results and train_label_recall_key in clf_results and \\\n",
    "            train_label_fscore_key in clf_results and train_label_support_key in clf_results:\n",
    "\n",
    "            train_report.loc[labels[label_id], :] = [clf_results[train_label_precision_key][i],\n",
    "                                                      clf_results[train_label_recall_key][i],\n",
    "                                                      clf_results[train_label_fscore_key][i],\n",
    "                                                      clf_results[train_label_support_key][i]]\n",
    "\n",
    "        test_label_precision_key = 'test_label' + str(label_id) + '_precision'\n",
    "        test_label_recall_key = 'test_label' + str(label_id) + '_recall'\n",
    "        test_label_fscore_key = 'test_label' + str(label_id) + '_fscore'\n",
    "        test_label_support_key = 'test_label' + str(label_id) + '_support'\n",
    "\n",
    "        if test_label_precision_key in clf_results and test_label_recall_key in clf_results and \\\n",
    "            test_label_fscore_key in clf_results and test_label_support_key in clf_results:\n",
    "\n",
    "            test_report.loc[labels[label_id], :] = [clf_results[test_label_precision_key][i],\n",
    "                                                     clf_results[test_label_recall_key][i],\n",
    "                                                     clf_results[test_label_fscore_key][i],\n",
    "                                                     clf_results[test_label_support_key][i]]\n",
    "\n",
    "    train_avg_precision_key = 'train_avg_precision'\n",
    "    train_avg_recall_key = 'train_avg_recall'\n",
    "    train_avg_fscore_key = 'train_avg_fscore'\n",
    "    train_total_support_key = 'train_total_support'\n",
    "\n",
    "    if train_avg_precision_key in clf_results and train_avg_recall_key in clf_results and \\\n",
    "        train_avg_fscore_key in clf_results and train_total_support_key in clf_results:\n",
    "\n",
    "        train_report.loc['Avg/Total', :] = [clf_results[train_avg_precision_key][i],\n",
    "                                             clf_results[train_avg_recall_key][i],\n",
    "                                             clf_results[train_avg_fscore_key][i],\n",
    "                                             clf_results[train_total_support_key][i]]\n",
    "\n",
    "    test_avg_precision_key = 'test_avg_precision'\n",
    "    test_avg_recall_key = 'test_avg_recall'\n",
    "    test_avg_fscore_key = 'test_avg_fscore'\n",
    "    test_total_support_key = 'test_total_support'\n",
    "\n",
    "    if test_avg_precision_key in clf_results and test_avg_recall_key in clf_results and \\\n",
    "        test_avg_fscore_key in clf_results and test_total_support_key in clf_results:\n",
    "\n",
    "        test_report.loc['Avg/Total', :] = [clf_results[test_avg_precision_key][i],\n",
    "                                            clf_results[test_avg_recall_key][i],\n",
    "                                            clf_results[test_avg_fscore_key][i],\n",
    "                                            clf_results[test_total_support_key][i]]\n",
    "\n",
    "    fold_index = pd.DataFrame(data=[{'Fold': 'Fold ' + str(i)}])\n",
    "    fold_index.to_excel(writer, 'LTC', startrow=datalength, index=False)\n",
    "    datalength += (len(fold_index) + 2)\n",
    "    train_report.to_excel(writer, 'LTC', startrow=datalength)\n",
    "    datalength += (len(train_report) + 2)\n",
    "    test_report.to_excel(writer, 'LTC', startrow=datalength)\n",
    "    datalength += (len(test_report) + 2)\n",
    "\n",
    "    result_dict['LTC_train_' + str(i)] = train_report\n",
    "    result_dict['LTC_test_' + str(i)] = test_report\n",
    "\n",
    "    train_report = train_report.astype(float).round(2)\n",
    "    test_report = test_report.astype(float).round(2)\n",
    "\n",
    "    print(\"\\n------------------------- FOLD \" + str(i) + \": -------------------------\")\n",
    "    print(\"\\nTraining Results:\")\n",
    "    print(train_report)\n",
    "    print(\"\\nTest Results:\")\n",
    "    print(test_report)\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Test Report Across 5 Folds:\n",
      "                                   Avg Precision  Avg Recall  Avg F1-score  \\\n",
      "Action on Issue                             0.08        1.00          0.14   \n",
      "Bug Reproduction                            0.00        0.00          0.00   \n",
      "Contribution and Commitment                 0.00        0.00          0.00   \n",
      "Expected Behaviour                          0.00        0.00          0.00   \n",
      "Investigation and Exploration               0.00        0.00          0.00   \n",
      "Motivation                                  0.00        0.00          0.00   \n",
      "Observed Bug Behaviour                      0.00        0.00          0.00   \n",
      "Potential New Issues and Requests           0.00        0.00          0.00   \n",
      "Social Conversation                         0.00        0.00          0.00   \n",
      "Solution Discussion                         0.00        0.00          0.00   \n",
      "Solution Usage                              0.00        0.00          0.00   \n",
      "Task Progress                               0.00        0.00          0.00   \n",
      "Workarounds                                 0.00        0.00          0.00   \n",
      "Total/Avg                                   0.01        0.08          0.01   \n",
      "\n",
      "                                   Avg Support  \n",
      "Action on Issue                          590.0  \n",
      "Bug Reproduction                         590.0  \n",
      "Contribution and Commitment              590.0  \n",
      "Expected Behaviour                       590.0  \n",
      "Investigation and Exploration            590.0  \n",
      "Motivation                               590.0  \n",
      "Observed Bug Behaviour                   590.0  \n",
      "Potential New Issues and Requests        590.0  \n",
      "Social Conversation                      590.0  \n",
      "Solution Discussion                      590.0  \n",
      "Solution Usage                           590.0  \n",
      "Task Progress                            590.0  \n",
      "Workarounds                              590.0  \n",
      "Total/Avg                               7670.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "avg_test_report = pd.DataFrame(columns=['Avg Precision', 'Avg Recall', 'Avg F1-score', 'Avg Support'], index=labels)\n",
    "\n",
    "# Variables to calculate weighted averages\n",
    "total_test_support = 0\n",
    "weighted_test_precision = 0\n",
    "weighted_test_recall = 0\n",
    "weighted_test_f1 = 0\n",
    "\n",
    "# Calculate averages across 5 folds for each label\n",
    "for label_id in range(13):\n",
    "    test_precisions = [result_dict['LTC_test_' + str(i)].loc[labels[label_id], 'Precision'] for i in range(5)]\n",
    "    test_recalls = [result_dict['LTC_test_' + str(i)].loc[labels[label_id], 'Recall'] for i in range(5)]\n",
    "    test_f1_scores = [result_dict['LTC_test_' + str(i)].loc[labels[label_id], 'F1-score'] for i in range(5)]\n",
    "    test_supports = [result_dict['LTC_test_' + str(i)].loc[labels[label_id], 'Support'] for i in range(5)]\n",
    "\n",
    "    # Calculate averages for each metric\n",
    "    avg_test_precision = np.mean(test_precisions)\n",
    "    avg_test_recall = np.mean(test_recalls)\n",
    "    avg_test_f1 = np.mean(test_f1_scores)\n",
    "    avg_test_support = np.mean(test_supports)  \n",
    "\n",
    "    avg_test_report.loc[labels[label_id]] = [avg_test_precision, avg_test_recall, avg_test_f1, avg_test_support]\n",
    "\n",
    "    weighted_test_precision += avg_test_precision * avg_test_support\n",
    "    weighted_test_recall += avg_test_recall * avg_test_support\n",
    "    weighted_test_f1 += avg_test_f1 * avg_test_support\n",
    "    total_test_support += avg_test_support\n",
    "\n",
    "if total_test_support > 0:\n",
    "    avg_test_report.loc['Total/Avg', :] = [weighted_test_precision / total_test_support,\n",
    "                                           weighted_test_recall / total_test_support,\n",
    "                                           weighted_test_f1 / total_test_support,\n",
    "                                           total_test_support]\n",
    "\n",
    "avg_test_report = avg_test_report.astype(float).round(2)\n",
    "\n",
    "existing_file_path = '../results/hyperparameter_result5.xlsx'  \n",
    "try:\n",
    "    existing_data = pd.read_excel(existing_file_path, sheet_name='LTC')  \n",
    "    startrow = len(existing_data) + 2  \n",
    "except FileNotFoundError:\n",
    "    startrow = 0  \n",
    "\n",
    "with pd.ExcelWriter(existing_file_path, mode='a', engine='openpyxl', if_sheet_exists='overlay') as writer:\n",
    "    avg_test_report.to_excel(writer, sheet_name='LTC', startrow=startrow, index=True)  \n",
    "\n",
    "\n",
    "print(\"\\nAverage Test Report Across 5 Folds:\")\n",
    "print(avg_test_report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
