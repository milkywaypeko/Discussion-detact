[{
  "url": "https://api.github.com/repos/explosion/spaCy/issues/285",
  "repository_url": "https://api.github.com/repos/explosion/spaCy",
  "labels_url": "https://api.github.com/repos/explosion/spaCy/issues/285/labels{/name}",
  "comments_url": "https://api.github.com/repos/explosion/spaCy/issues/285/comments",
  "events_url": "https://api.github.com/repos/explosion/spaCy/issues/285/events",
  "html_url": "https://github.com/explosion/spaCy/issues/285",
  "id": 139959710,
  "node_id": "MDU6SXNzdWUxMzk5NTk3MTA=",
  "number": 285,
  "title": "Streaming Data Memory Growth",
  "user": {
    "login": "ELind77",
    "id": 5109720,
    "node_id": "MDQ6VXNlcjUxMDk3MjA=",
    "avatar_url": "https://avatars1.githubusercontent.com/u/5109720?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/ELind77",
    "html_url": "https://github.com/ELind77",
    "followers_url": "https://api.github.com/users/ELind77/followers",
    "following_url": "https://api.github.com/users/ELind77/following{/other_user}",
    "gists_url": "https://api.github.com/users/ELind77/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/ELind77/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/ELind77/subscriptions",
    "organizations_url": "https://api.github.com/users/ELind77/orgs",
    "repos_url": "https://api.github.com/users/ELind77/repos",
    "events_url": "https://api.github.com/users/ELind77/events{/privacy}",
    "received_events_url": "https://api.github.com/users/ELind77/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [
    {
      "id": 111380485,
      "node_id": "MDU6TGFiZWwxMTEzODA0ODU=",
      "url": "https://api.github.com/repos/explosion/spaCy/labels/bug",
      "name": "bug",
      "color": "DD2A27",
      "default": true
    }
  ],
  "state": "closed",
  "locked": true,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 37,
  "created_at": "2016-03-10T17:34:10Z",
  "updated_at": "2018-05-08T14:27:38Z",
  "closed_at": "2017-10-16T18:20:36Z",
  "author_association": "NONE",
  "body": "I have been using spacy for streaming data (twitter and news stories mostly) and I believe that the fundamental design of the vocab/StringStore in spacy is problematic for streaming processing.  When used for batch jobs the additional memory overhead of storing a new lexeme struct for each new word form encountered in parsing is negligible compared to the speed gains, and because most text conforms to the assumption that vocabulary size grows logarithmically as the total number of tokens grows linearly this is usually a safe bet.  But for streaming text, especially for social media where new terms are invented by the minute (hashtags and URLs in particular) this assumption no longer holds and the spacy vocabulary storage represents a dynamic element in what should be a completely static production deployment.   \n\nIn order to test this assumption, I took one million tweets and performed a rudimentary analysis using the resources module in python to get the maximum memory used by the program at regular intervals during processing.  I first performed some minor preprocessing to remove newlines from the data so that it could be read line by line so that it wouldn't all be kept in memory, then I ran spacy with all models set to false, only the tokenizer loaded.  I then did the same thing again after removing all URLs, hashtags, and twitter mentions from the data , and then filtering all empty strings (this resulted in a 1.4% data loss in terms of total tweets processed but that's fairly minor).  \n\nThe final result was that spacy used an additional 278.6 MB after tokenizing the raw tweets and 60.99 MB of additional memory when tokenizing the pre-processed tweets.  This result confirms my hypothesis but also shows that the memory increase really isn't all that significant (especially at the relatively low volume that I am currently processing).  But it still points to a potential flaw in the design of the library.   \n\nMy suggestion/request in the near term would be to have an option to make the vocabulary read only so that users who want to be able to leave spacy alone to do streaming data processing don't need to worry about changing memory requirements.  In the long term, I think that an optimal solution would be to add some functionality for a timeout on vocabulary entries that aren't loaded at initialization.  E.g. if this lexeme hasn't been accessed for the last _n_ seconds, delete it from the StringStore.  And _n_ would be user configurable. \n\nMy code and results are available here: https://github.com/ELind77/spacy_memory_growth \n\nThanks again for continuing to develop such a great library! \n\n-- Eric \n",
  "closed_by": {
    "login": "honnibal",
    "id": 8059750,
    "node_id": "MDQ6VXNlcjgwNTk3NTA=",
    "avatar_url": "https://avatars3.githubusercontent.com/u/8059750?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/honnibal",
    "html_url": "https://github.com/honnibal",
    "followers_url": "https://api.github.com/users/honnibal/followers",
    "following_url": "https://api.github.com/users/honnibal/following{/other_user}",
    "gists_url": "https://api.github.com/users/honnibal/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/honnibal/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/honnibal/subscriptions",
    "organizations_url": "https://api.github.com/users/honnibal/orgs",
    "repos_url": "https://api.github.com/users/honnibal/repos",
    "events_url": "https://api.github.com/users/honnibal/events{/privacy}",
    "received_events_url": "https://api.github.com/users/honnibal/received_events",
    "type": "User",
    "site_admin": false
  }
},{"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/195059590", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-195059590", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 195059590, "node_id": "MDEyOklzc3VlQ29tbWVudDE5NTA1OTU5MA==", "user": {"login": "honnibal", "id": 8059750, "node_id": "MDQ6VXNlcjgwNTk3NTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8059750?v=4", "gravatar_id": "", "url": "https://api.github.com/users/honnibal", "html_url": "https://github.com/honnibal", "followers_url": "https://api.github.com/users/honnibal/followers", "following_url": "https://api.github.com/users/honnibal/following{/other_user}", "gists_url": "https://api.github.com/users/honnibal/gists{/gist_id}", "starred_url": "https://api.github.com/users/honnibal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/honnibal/subscriptions", "organizations_url": "https://api.github.com/users/honnibal/orgs", "repos_url": "https://api.github.com/users/honnibal/repos", "events_url": "https://api.github.com/users/honnibal/events{/privacy}", "received_events_url": "https://api.github.com/users/honnibal/received_events", "type": "User", "site_admin": false}, "created_at": "2016-03-10T21:36:04Z", "updated_at": "2016-03-10T21:36:04Z", "author_association": "MEMBER", "body": "I really need to fix this issue. Thanks.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/196981854", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-196981854", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 196981854, "node_id": "MDEyOklzc3VlQ29tbWVudDE5Njk4MTg1NA==", "user": {"login": "syllog1sm", "id": 781165, "node_id": "MDQ6VXNlcjc4MTE2NQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/781165?v=4", "gravatar_id": "", "url": "https://api.github.com/users/syllog1sm", "html_url": "https://github.com/syllog1sm", "followers_url": "https://api.github.com/users/syllog1sm/followers", "following_url": "https://api.github.com/users/syllog1sm/following{/other_user}", "gists_url": "https://api.github.com/users/syllog1sm/gists{/gist_id}", "starred_url": "https://api.github.com/users/syllog1sm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/syllog1sm/subscriptions", "organizations_url": "https://api.github.com/users/syllog1sm/orgs", "repos_url": "https://api.github.com/users/syllog1sm/repos", "events_url": "https://api.github.com/users/syllog1sm/events{/privacy}", "received_events_url": "https://api.github.com/users/syllog1sm/received_events", "type": "User", "site_admin": false}, "created_at": "2016-03-15T19:18:30Z", "updated_at": "2016-03-15T19:18:30Z", "author_association": "COLLABORATOR", "body": "I think this patch from a few weeks ago might have fixed this. Unfortunately the patch wasn't pushed to master --- I've only just merged it now.\n\nThe fix is: https://github.com/spacy-io/spaCy/commit/141639ea3a8fc65f006d16594ab207b3a1c8c7d5\n\nCould you test this by `pip install https://github.com/spacy-io/spaCy/archive/master.zip` ?\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/199492483", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-199492483", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 199492483, "node_id": "MDEyOklzc3VlQ29tbWVudDE5OTQ5MjQ4Mw==", "user": {"login": "ELind77", "id": 5109720, "node_id": "MDQ6VXNlcjUxMDk3MjA=", "avatar_url": "https://avatars1.githubusercontent.com/u/5109720?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ELind77", "html_url": "https://github.com/ELind77", "followers_url": "https://api.github.com/users/ELind77/followers", "following_url": "https://api.github.com/users/ELind77/following{/other_user}", "gists_url": "https://api.github.com/users/ELind77/gists{/gist_id}", "starred_url": "https://api.github.com/users/ELind77/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ELind77/subscriptions", "organizations_url": "https://api.github.com/users/ELind77/orgs", "repos_url": "https://api.github.com/users/ELind77/repos", "events_url": "https://api.github.com/users/ELind77/events{/privacy}", "received_events_url": "https://api.github.com/users/ELind77/received_events", "type": "User", "site_admin": false}, "created_at": "2016-03-21T21:23:40Z", "updated_at": "2016-03-21T21:23:40Z", "author_association": "NONE", "body": "Sorry this has taken a while.  I'll test again today/tomorrow and get back to you. \n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/201612480", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-201612480", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 201612480, "node_id": "MDEyOklzc3VlQ29tbWVudDIwMTYxMjQ4MA==", "user": {"login": "ELind77", "id": 5109720, "node_id": "MDQ6VXNlcjUxMDk3MjA=", "avatar_url": "https://avatars1.githubusercontent.com/u/5109720?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ELind77", "html_url": "https://github.com/ELind77", "followers_url": "https://api.github.com/users/ELind77/followers", "following_url": "https://api.github.com/users/ELind77/following{/other_user}", "gists_url": "https://api.github.com/users/ELind77/gists{/gist_id}", "starred_url": "https://api.github.com/users/ELind77/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ELind77/subscriptions", "organizations_url": "https://api.github.com/users/ELind77/orgs", "repos_url": "https://api.github.com/users/ELind77/repos", "events_url": "https://api.github.com/users/ELind77/events{/privacy}", "received_events_url": "https://api.github.com/users/ELind77/received_events", "type": "User", "site_admin": false}, "created_at": "2016-03-26T00:25:04Z", "updated_at": "2016-03-26T00:25:04Z", "author_association": "NONE", "body": "I performed the same tests again, both installing from the zip you posted above and installing directly from master (commit 9cd21ad5b5aa664642a2e17925cd7b39eacb9aa9) and got nearly identical results to my previous trials.  \n\nIf you believe that this is the cause of some kind of memory leak, I think we should really take a look at my testing script and update it as it's very rudimentary and I'm far from an expert profiler.  However, I don't think that this is a leak.  As I said in my original post, I think that this is just part of how spacy works.  When parsing things like social media where there are many tokens that occur only once (e.g. links) storing them in the StringStore causes memory bloat.  In your comments on https://github.com/spacy-io/spaCy/issues/172 you proposed a batch-processing generator that uses, and then throws away a tokenizer object for each batch in order to help find OOV tokens.  I think that's a fine approach, and could even be done a bit more quickly by asynchronously loading the new English() instance and replacing the old one when the new is ready, but that still leads to quite the slow down.  \n\nIf your feeling is that spacy is really meant for batch processing and that I should use mini-batches if I want to approximate streaming, I can do that.  Spacy is still far superior to anything else out there in my opinion, but it would be nice if I could use it with the expectation of roughly constant space complexity.  \n\n-- Eric\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/202697335", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-202697335", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 202697335, "node_id": "MDEyOklzc3VlQ29tbWVudDIwMjY5NzMzNQ==", "user": {"login": "honnibal", "id": 8059750, "node_id": "MDQ6VXNlcjgwNTk3NTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8059750?v=4", "gravatar_id": "", "url": "https://api.github.com/users/honnibal", "html_url": "https://github.com/honnibal", "followers_url": "https://api.github.com/users/honnibal/followers", "following_url": "https://api.github.com/users/honnibal/following{/other_user}", "gists_url": "https://api.github.com/users/honnibal/gists{/gist_id}", "starred_url": "https://api.github.com/users/honnibal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/honnibal/subscriptions", "organizations_url": "https://api.github.com/users/honnibal/orgs", "repos_url": "https://api.github.com/users/honnibal/repos", "events_url": "https://api.github.com/users/honnibal/events{/privacy}", "received_events_url": "https://api.github.com/users/honnibal/received_events", "type": "User", "site_admin": false}, "created_at": "2016-03-29T03:52:16Z", "updated_at": "2016-03-29T03:52:16Z", "author_association": "MEMBER", "body": "To clarify a little bit, the current release version has three known places that could be growing in memory use.\n1. The `StringStore`\n2. A cache in the `Tokenizer`\n3. The `Vocab`, for tokens that are part of `prefix`, `infix` and `suffix` patterns.\n\nThe patch I asked you to try out addresses 3. We can also easily address 2. Addressing 1 is hard, because we currently intern all the strings, which is a much easier policy to implement than something more subtle.\n\nCan you report the lengths of the `StringStore` class in your two benchmark cases? There's currently no Python API for inspecting the size of the tokenizer's cache, so it's easiest to do this by elimination.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/238594970", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-238594970", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 238594970, "node_id": "MDEyOklzc3VlQ29tbWVudDIzODU5NDk3MA==", "user": {"login": "ptully", "id": 11786363, "node_id": "MDQ6VXNlcjExNzg2MzYz", "avatar_url": "https://avatars1.githubusercontent.com/u/11786363?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ptully", "html_url": "https://github.com/ptully", "followers_url": "https://api.github.com/users/ptully/followers", "following_url": "https://api.github.com/users/ptully/following{/other_user}", "gists_url": "https://api.github.com/users/ptully/gists{/gist_id}", "starred_url": "https://api.github.com/users/ptully/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ptully/subscriptions", "organizations_url": "https://api.github.com/users/ptully/orgs", "repos_url": "https://api.github.com/users/ptully/repos", "events_url": "https://api.github.com/users/ptully/events{/privacy}", "received_events_url": "https://api.github.com/users/ptully/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-09T15:42:48Z", "updated_at": "2016-08-09T15:42:48Z", "author_association": "NONE", "body": "Hi @honnibal, why do you think addressing 1 is so hard? What about FIFO queue or similar, or something like @ELind77 suggested like:\n\n> functionality for a timeout on vocabulary entries that aren't loaded at initialization\n\nDo you have any more information on this issue since it cropped up a few months ago? I notice the same type of memory issues on my systems that analyze streaming Twitter data - note I've not yet narrowed it down to spacy yet but my first cursory look found this ticket to be the most relevant possibility\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/238595428", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-238595428", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 238595428, "node_id": "MDEyOklzc3VlQ29tbWVudDIzODU5NTQyOA==", "user": {"login": "ptully", "id": 11786363, "node_id": "MDQ6VXNlcjExNzg2MzYz", "avatar_url": "https://avatars1.githubusercontent.com/u/11786363?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ptully", "html_url": "https://github.com/ptully", "followers_url": "https://api.github.com/users/ptully/followers", "following_url": "https://api.github.com/users/ptully/following{/other_user}", "gists_url": "https://api.github.com/users/ptully/gists{/gist_id}", "starred_url": "https://api.github.com/users/ptully/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ptully/subscriptions", "organizations_url": "https://api.github.com/users/ptully/orgs", "repos_url": "https://api.github.com/users/ptully/repos", "events_url": "https://api.github.com/users/ptully/events{/privacy}", "received_events_url": "https://api.github.com/users/ptully/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-09T15:44:20Z", "updated_at": "2016-08-09T15:44:20Z", "author_association": "NONE", "body": "Also curious if this issue is already solved already, I will test updating my version (currently 0.100.6) to see if that helps at all\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/240198422", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-240198422", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 240198422, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MDE5ODQyMg==", "user": {"login": "natb1", "id": 1669062, "node_id": "MDQ6VXNlcjE2NjkwNjI=", "avatar_url": "https://avatars0.githubusercontent.com/u/1669062?v=4", "gravatar_id": "", "url": "https://api.github.com/users/natb1", "html_url": "https://github.com/natb1", "followers_url": "https://api.github.com/users/natb1/followers", "following_url": "https://api.github.com/users/natb1/following{/other_user}", "gists_url": "https://api.github.com/users/natb1/gists{/gist_id}", "starred_url": "https://api.github.com/users/natb1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/natb1/subscriptions", "organizations_url": "https://api.github.com/users/natb1/orgs", "repos_url": "https://api.github.com/users/natb1/repos", "events_url": "https://api.github.com/users/natb1/events{/privacy}", "received_events_url": "https://api.github.com/users/natb1/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-16T18:43:57Z", "updated_at": "2016-08-16T18:43:57Z", "author_association": "NONE", "body": "Hi @honnibal, I have had similar issues in my streaming application. Basically memory grows at a logarithmic-ish pace. We have to deal with it as though it were a memory leak and periodically re-initialize the code.\n\nI ran the benchmark you requested above - collecting metrics on the length of the `StringStore` as memory usage grows. Here are the results:\n<img width=\"555\" alt=\"screen shot 2016-08-16 at 2 24 25 pm\" src=\"https://cloud.githubusercontent.com/assets/1669062/17711263/c23cb262-63be-11e6-9aaf-96e9763a00e0.png\">\n\nHere is the code I used to create the metrics. It basically ran until I ran out of memory on a 4G box.  https://github.com/natb1/spaCy/blob/memory-benchmark/spacy/tests/benchmark/test_memory.py\n\nI'd be glad to help implement some strategies to address this problem if you could help me isolate the issue and/or suggest some approaches.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/241865509", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-241865509", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 241865509, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MTg2NTUwOQ==", "user": {"login": "tomtung", "id": 513210, "node_id": "MDQ6VXNlcjUxMzIxMA==", "avatar_url": "https://avatars3.githubusercontent.com/u/513210?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tomtung", "html_url": "https://github.com/tomtung", "followers_url": "https://api.github.com/users/tomtung/followers", "following_url": "https://api.github.com/users/tomtung/following{/other_user}", "gists_url": "https://api.github.com/users/tomtung/gists{/gist_id}", "starred_url": "https://api.github.com/users/tomtung/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tomtung/subscriptions", "organizations_url": "https://api.github.com/users/tomtung/orgs", "repos_url": "https://api.github.com/users/tomtung/repos", "events_url": "https://api.github.com/users/tomtung/events{/privacy}", "received_events_url": "https://api.github.com/users/tomtung/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-23T20:24:17Z", "updated_at": "2016-08-23T20:24:17Z", "author_association": "CONTRIBUTOR", "body": "Same problem here. Would also be glad to help.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/242388516", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-242388516", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 242388516, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MjM4ODUxNg==", "user": {"login": "ptully", "id": 11786363, "node_id": "MDQ6VXNlcjExNzg2MzYz", "avatar_url": "https://avatars1.githubusercontent.com/u/11786363?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ptully", "html_url": "https://github.com/ptully", "followers_url": "https://api.github.com/users/ptully/followers", "following_url": "https://api.github.com/users/ptully/following{/other_user}", "gists_url": "https://api.github.com/users/ptully/gists{/gist_id}", "starred_url": "https://api.github.com/users/ptully/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ptully/subscriptions", "organizations_url": "https://api.github.com/users/ptully/orgs", "repos_url": "https://api.github.com/users/ptully/repos", "events_url": "https://api.github.com/users/ptully/events{/privacy}", "received_events_url": "https://api.github.com/users/ptully/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-25T13:36:45Z", "updated_at": "2016-08-25T13:36:45Z", "author_association": "NONE", "body": "pinging @henningpeters given recent announcement on spaCy homepage\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/242420597", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-242420597", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 242420597, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MjQyMDU5Nw==", "user": {"login": "honnibal", "id": 8059750, "node_id": "MDQ6VXNlcjgwNTk3NTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8059750?v=4", "gravatar_id": "", "url": "https://api.github.com/users/honnibal", "html_url": "https://github.com/honnibal", "followers_url": "https://api.github.com/users/honnibal/followers", "following_url": "https://api.github.com/users/honnibal/following{/other_user}", "gists_url": "https://api.github.com/users/honnibal/gists{/gist_id}", "starred_url": "https://api.github.com/users/honnibal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/honnibal/subscriptions", "organizations_url": "https://api.github.com/users/honnibal/orgs", "repos_url": "https://api.github.com/users/honnibal/repos", "events_url": "https://api.github.com/users/honnibal/events{/privacy}", "received_events_url": "https://api.github.com/users/honnibal/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-25T15:03:04Z", "updated_at": "2016-08-25T15:03:04Z", "author_association": "MEMBER", "body": "To clarify the current behaviour a little: `StringStore` is currently interning _all_ strings seen. I agree that this should be changed. I'll discuss the design decision here, so that we can consider the trade-offs.\n\nI'll start from the beginning: why intern the strings? Two main reasons:\n\n1) String-to-int mapping\n\n2) Save memory to represent lots of documents at once.\n\nWe can't do without 1 entirely \u2014 it's too fundamental to how spaCy is working, and we definitely don't want to be making lots of string comparisons. Comparing by integer value is pretty important.\n\nConsideration 2 is very useful, but it's only really a saving if strings occur multiple times. Certainly, for strings that occur once, there's no advantage. And it's also bad to have unbounded memory use on the streaming process.\n\nSo the solution we want to get to is one where a limited number of somewhat common strings are interned in the common vocab. However, we still need to map _all_ strings, even rare ones, to integers. We also want the string-to-int table to be consistent, even for rare strings.\n\nHere's the bit of code where the memory growth is occuring:\n\nhttps://github.com/spacy-io/spaCy/blob/master/spacy/strings.pyx#L147\n\nThe purpose here is to resolve a string to an integer. We first hash the string. Now, this is an integer representation --- so why not just use the hash? The problem is we also want the inverse mapping. We therefore store the string, causing the memory growth.\n\nIf we insist that all integers can always be mapped back to strings, there's no solution. We have to accept the memory growth. But if we can accept that these strings pass out of date, so that they're around for a while and then they're not, the situation should be manageable.\n\nA simple way to achieve this is to extend the `StringStore` so that the mapping is split in two. There's the main intern area, which holds a fixed number of strings, hopefully the common ones. But there's also a rolling buffer, in which strings are interned, and then later freed. This could be a LRU cache, or even something simpler. Efficiency is not really a problem here: only a small percentage of the encountered tokens will be triggering this logic, so we don't have to make it blazing fast, and it's easy to make sure we operate on contiguous buffers.\n\nA slightly more tricky solution is to do some reference counting. The idea here would be for the `Doc` object to register interest in all OOV strings it owns. When the ref count of an OOV string drops to 0, it's freed. This way, if you keep a `Doc` object in memory, you know that the string lookup will always be well behaved \u2014 but if you're letting the `Doc` objects pass out of scope, your memory won't be growing.\n\nI think for both solutions, we should use the hash of the string as the integer representation for OOV strings. This means that at least the string-to-int mapping will stay consistent, even if strings are passing out of memory. The only way to have a problem here is if you hold onto the integer representation, release all of the documents, and later want to recover the string. In this situation, you'll be out of luck --- but we'll at least know to use an OOV symbol when you try to look up your string.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/242951985", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-242951985", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 242951985, "node_id": "MDEyOklzc3VlQ29tbWVudDI0Mjk1MTk4NQ==", "user": {"login": "tomtung", "id": 513210, "node_id": "MDQ6VXNlcjUxMzIxMA==", "avatar_url": "https://avatars3.githubusercontent.com/u/513210?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tomtung", "html_url": "https://github.com/tomtung", "followers_url": "https://api.github.com/users/tomtung/followers", "following_url": "https://api.github.com/users/tomtung/following{/other_user}", "gists_url": "https://api.github.com/users/tomtung/gists{/gist_id}", "starred_url": "https://api.github.com/users/tomtung/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tomtung/subscriptions", "organizations_url": "https://api.github.com/users/tomtung/orgs", "repos_url": "https://api.github.com/users/tomtung/repos", "events_url": "https://api.github.com/users/tomtung/events{/privacy}", "received_events_url": "https://api.github.com/users/tomtung/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-28T02:18:34Z", "updated_at": "2016-08-28T02:18:34Z", "author_association": "CONTRIBUTOR", "body": "Simply hashing oov tokens sounds good enough to me. As long as we know the\nindices of the fist and last characters of the token in the input text, so\nthat we can look it up if we need to, I don't find saving the token in\nstring store particularly necessary.\n\nOn Thu, Aug 25, 2016, 8:03 AM Matthew Honnibal notifications@github.com\nwrote:\n\n> To clarify the current behaviour a little: StringStore is currently\n> interning _all_ strings seen. I agree that this should be changed. I'll\n> discuss the design decision here, so that we can consider the trade-offs.\n> \n> I'll start from the beginning: why intern the strings? Two main reasons:\n> \n> 1) String-to-int mapping\n> \n> 2) Save memory to represent lots of documents at once.\n> \n> We can't do without 1 entirely \u2014 it's too fundamental to how spaCy is\n> working, and we definitely don't want to be making lots of string\n> comparisons. Comparing by integer value is pretty important.\n> \n> Consideration 2 is very useful, but it's only really a saving if strings\n> occur multiple times. Certainly, for strings that occur once, there's no\n> advantage. And it's also bad to have unbounded memory use on the streaming\n> process.\n> \n> So the solution we want to get to is one where a limited number of\n> somewhat common strings are interned in the common vocab. However, we still\n> need to map _all_ strings, even rare ones, to integers. We also want the\n> string-to-int table to be consistent, even for rare strings.\n> \n> Here's the bit of code where the memory growth is occuring:\n> \n> https://github.com/spacy-io/spaCy/blob/master/spacy/strings.pyx#L147\n> \n> The purpose here is to resolve a string to an integer. We first hash the\n> string. Now, this is an integer representation --- so why not just use the\n> hash? The problem is we also want the inverse mapping. We therefore store\n> the string, causing the memory growth.\n> \n> If we insist that all integers can always be mapped back to strings,\n> there's no solution. We have to accept the memory growth. But if we can\n> accept that these strings pass out of date, so that they're around for a\n> while and then they're not, the situation should be manageable.\n> \n> A simple way to achieve this is to extend the StringStore so that the\n> mapping is split in two. There's the main intern area, which holds a fixed\n> number of strings, hopefully the common ones. But there's also a rolling\n> buffer, in which strings are interned, and then later freed. This could be\n> a LRU cache, or even something simpler. Efficiency is not really a problem\n> here: only a small percentage of the encountered tokens will be triggering\n> this logic, so we don't have to make it blazing fast, and it's easy to make\n> sure we operate on contiguous buffers.\n> \n> A slightly more tricky solution is to do some reference counting. The idea\n> here would be for the Doc object to register interest in all OOV strings\n> it owns. When the ref count of an OOV string drops to 0, it's freed. This\n> way, if you keep a Doc object in memory, you know that the string lookup\n> will always be well behaved \u2014 but if you're letting the Doc objects pass\n> out of scope, your memory won't be growing.\n> \n> I think for both solutions, we should use the hash of the string as the\n> integer representation for OOV strings. This means that at least the\n> string-to-int mapping will stay consistent, even if strings are passing out\n> of memory. The only way to have a problem here is if you hold onto the\n> integer representation, release all of the documents, and later want to\n> recover the string. In this situation, you'll be out of luck --- but we'll\n> at least know to use an OOV symbol when you try to look up your string.\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/spacy-io/spaCy/issues/285#issuecomment-242420597, or mute\n> the thread\n> https://github.com/notifications/unsubscribe-auth/AAfUutsOJ17JS8IRWp2pZNtc6BmNbExsks5qja6tgaJpZM4Ht9oH\n> .\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/242963939", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-242963939", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 242963939, "node_id": "MDEyOklzc3VlQ29tbWVudDI0Mjk2MzkzOQ==", "user": {"login": "honnibal", "id": 8059750, "node_id": "MDQ6VXNlcjgwNTk3NTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8059750?v=4", "gravatar_id": "", "url": "https://api.github.com/users/honnibal", "html_url": "https://github.com/honnibal", "followers_url": "https://api.github.com/users/honnibal/followers", "following_url": "https://api.github.com/users/honnibal/following{/other_user}", "gists_url": "https://api.github.com/users/honnibal/gists{/gist_id}", "starred_url": "https://api.github.com/users/honnibal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/honnibal/subscriptions", "organizations_url": "https://api.github.com/users/honnibal/orgs", "repos_url": "https://api.github.com/users/honnibal/repos", "events_url": "https://api.github.com/users/honnibal/events{/privacy}", "received_events_url": "https://api.github.com/users/honnibal/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-28T08:56:24Z", "updated_at": "2016-08-28T08:56:24Z", "author_association": "MEMBER", "body": "The input text is currently not saved/represented on the document at the moment. Instead, we guarantee that the `orth` attribute faithfully retains the slice for each token, so that we just have to join the `orth` attributes and check whether the token has a trailing space.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/242964155", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-242964155", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 242964155, "node_id": "MDEyOklzc3VlQ29tbWVudDI0Mjk2NDE1NQ==", "user": {"login": "tomtung", "id": 513210, "node_id": "MDQ6VXNlcjUxMzIxMA==", "avatar_url": "https://avatars3.githubusercontent.com/u/513210?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tomtung", "html_url": "https://github.com/tomtung", "followers_url": "https://api.github.com/users/tomtung/followers", "following_url": "https://api.github.com/users/tomtung/following{/other_user}", "gists_url": "https://api.github.com/users/tomtung/gists{/gist_id}", "starred_url": "https://api.github.com/users/tomtung/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tomtung/subscriptions", "organizations_url": "https://api.github.com/users/tomtung/orgs", "repos_url": "https://api.github.com/users/tomtung/repos", "events_url": "https://api.github.com/users/tomtung/events{/privacy}", "received_events_url": "https://api.github.com/users/tomtung/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-28T09:02:27Z", "updated_at": "2016-08-28T09:02:27Z", "author_association": "CONTRIBUTOR", "body": "Yeah I understand. The point I was making was that, since the caller of the `Language` object has the full input text string anyways, it shouldn't be a big problem to deal with the slight inconvenience of having to look up the original substring of OOV tokens.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/242965481", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-242965481", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 242965481, "node_id": "MDEyOklzc3VlQ29tbWVudDI0Mjk2NTQ4MQ==", "user": {"login": "honnibal", "id": 8059750, "node_id": "MDQ6VXNlcjgwNTk3NTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8059750?v=4", "gravatar_id": "", "url": "https://api.github.com/users/honnibal", "html_url": "https://github.com/honnibal", "followers_url": "https://api.github.com/users/honnibal/followers", "following_url": "https://api.github.com/users/honnibal/following{/other_user}", "gists_url": "https://api.github.com/users/honnibal/gists{/gist_id}", "starred_url": "https://api.github.com/users/honnibal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/honnibal/subscriptions", "organizations_url": "https://api.github.com/users/honnibal/orgs", "repos_url": "https://api.github.com/users/honnibal/repos", "events_url": "https://api.github.com/users/honnibal/events{/privacy}", "received_events_url": "https://api.github.com/users/honnibal/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-28T09:37:55Z", "updated_at": "2016-08-28T09:37:55Z", "author_association": "MEMBER", "body": "I think we're in agreement here. However, I think it's important that we either assume the string is unavailable, or save it ourselves.\n\nSaving the string on the document isn't a huge waste of memory, and it only impacts the API in a few places (e.g., `doc.from_array`, deserialisation, etc). So if we want the user to be slicing into the string ever, we should probably switch to saving it.\n\nHere's a design that achieves something like the reference counting:\n- Add a `oov_stores` member to `StringStore`, which will be a sequence of `StringStore` instances.\n- Already in `Vocab.get`, we accept a `Pool` argument, that represents the allocation pool that will own the memory for the created `LexemeC` struct. This allows `Doc` objects to own their OOV lexemes. We need to extend this such that the document also owns the strings. Relevant code in `Vocab`: https://github.com/spacy-io/spaCy/blob/master/spacy/vocab.pyx#L149 (called by `Vocab.get()`, called by `Tokenizer._attach_tokens()`)\n- I suggest using `id(mem)` as a way of selecting the appropriate child oov store. This will allow us to have a method `get_oov_string(string, store_id)` that can be called from the `Doc`, `Token` etc instances. \n- We then define a `Doc.__dealloc__` method, which is the Cython way of adding a destructor. In `Doc.__dealloc__`, we tell the `StringStore` to drop the oov store associated with the `Doc` object.\n- The `StringStore` remains a single source of truth for the string-to-integer mapping. When decoding an integer, we can search for it in all the OOV stores. This makes it easier to prevent integers from being \"stranded\".\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/243322072", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-243322072", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 243322072, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MzMyMjA3Mg==", "user": {"login": "tomtung", "id": 513210, "node_id": "MDQ6VXNlcjUxMzIxMA==", "avatar_url": "https://avatars3.githubusercontent.com/u/513210?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tomtung", "html_url": "https://github.com/tomtung", "followers_url": "https://api.github.com/users/tomtung/followers", "following_url": "https://api.github.com/users/tomtung/following{/other_user}", "gists_url": "https://api.github.com/users/tomtung/gists{/gist_id}", "starred_url": "https://api.github.com/users/tomtung/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tomtung/subscriptions", "organizations_url": "https://api.github.com/users/tomtung/orgs", "repos_url": "https://api.github.com/users/tomtung/repos", "events_url": "https://api.github.com/users/tomtung/events{/privacy}", "received_events_url": "https://api.github.com/users/tomtung/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-30T03:20:32Z", "updated_at": "2016-08-30T03:20:32Z", "author_association": "CONTRIBUTOR", "body": "This sounds great! Although probably due to lack of context and familiarity to the code base, I personally would still prefer some simpler approach that can keep the `StringStore` immutable. e.g. use hashing to map OOV tokens to ints, keep reference to the text string in the `Doc` object, and obtain `orth` strings by indexing on it. Maybe this immutability can help parallelizing other parts of the pipeline too.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/243369756", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-243369756", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 243369756, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MzM2OTc1Ng==", "user": {"login": "honnibal", "id": 8059750, "node_id": "MDQ6VXNlcjgwNTk3NTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8059750?v=4", "gravatar_id": "", "url": "https://api.github.com/users/honnibal", "html_url": "https://github.com/honnibal", "followers_url": "https://api.github.com/users/honnibal/followers", "following_url": "https://api.github.com/users/honnibal/following{/other_user}", "gists_url": "https://api.github.com/users/honnibal/gists{/gist_id}", "starred_url": "https://api.github.com/users/honnibal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/honnibal/subscriptions", "organizations_url": "https://api.github.com/users/honnibal/orgs", "repos_url": "https://api.github.com/users/honnibal/repos", "events_url": "https://api.github.com/users/honnibal/events{/privacy}", "received_events_url": "https://api.github.com/users/honnibal/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-30T08:29:36Z", "updated_at": "2016-08-30T08:29:36Z", "author_association": "MEMBER", "body": "Well, I think you could say \"the trap is set\": the existing design is such that the strings have to be globally available.\n\nRecall that we're allowing transport to/from numpy arrays. This means we're expecting to be able to unpack an array of ints and understand some of them as strings, without ties to a particular `Doc` object. This is the mechanism being used for deserialization.\n\nWe could hack through this by writing down the OOV strings in the global store only when we pack into an array. But I hope we can all agree that this is just digging ourselves a deeper hole. I would be very unhappy if I tried to pack an array myself in the obvious way, and I found that the library's version of this was quietly writing to global state, and without this write my method failed, but only on OOV words, so not on my test data!\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/243558738", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-243558738", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 243558738, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MzU1ODczOA==", "user": {"login": "tomtung", "id": 513210, "node_id": "MDQ6VXNlcjUxMzIxMA==", "avatar_url": "https://avatars3.githubusercontent.com/u/513210?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tomtung", "html_url": "https://github.com/tomtung", "followers_url": "https://api.github.com/users/tomtung/followers", "following_url": "https://api.github.com/users/tomtung/following{/other_user}", "gists_url": "https://api.github.com/users/tomtung/gists{/gist_id}", "starred_url": "https://api.github.com/users/tomtung/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tomtung/subscriptions", "organizations_url": "https://api.github.com/users/tomtung/orgs", "repos_url": "https://api.github.com/users/tomtung/repos", "events_url": "https://api.github.com/users/tomtung/events{/privacy}", "received_events_url": "https://api.github.com/users/tomtung/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-30T19:54:20Z", "updated_at": "2016-08-30T19:54:20Z", "author_association": "CONTRIBUTOR", "body": "> We could hack through this by writing down the OOV strings in the global store only when we pack into an array. But I hope we can all agree that this is just digging ourselves a deeper hole.\n\nYeah this sounds terrifying.\n\n> Recall that we're allowing transport to/from numpy arrays. This means we're expecting to be able to unpack an array of ints and understand some of them as strings, without ties to a particular Doc object. This is the mechanism being used for deserialization.\n\nI might be totally wrong, but I expect the feature of converting to/from numpy to only be used internally? The array doesn't seem to work across different `Language` instances if there're OOV tokens, which kind of defeats the purpose of serialization for normal users. So maybe we don't need to worry about breaking user code that uses it?\n\nI guess what I was proposing entails always including the original text as part of (de)serialization. This might be too much refactor work, in which case what you mentioned also sounds great :)\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/250686630", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-250686630", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 250686630, "node_id": "MDEyOklzc3VlQ29tbWVudDI1MDY4NjYzMA==", "user": {"login": "honnibal", "id": 8059750, "node_id": "MDQ6VXNlcjgwNTk3NTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8059750?v=4", "gravatar_id": "", "url": "https://api.github.com/users/honnibal", "html_url": "https://github.com/honnibal", "followers_url": "https://api.github.com/users/honnibal/followers", "following_url": "https://api.github.com/users/honnibal/following{/other_user}", "gists_url": "https://api.github.com/users/honnibal/gists{/gist_id}", "starred_url": "https://api.github.com/users/honnibal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/honnibal/subscriptions", "organizations_url": "https://api.github.com/users/honnibal/orgs", "repos_url": "https://api.github.com/users/honnibal/repos", "events_url": "https://api.github.com/users/honnibal/events{/privacy}", "received_events_url": "https://api.github.com/users/honnibal/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-30T08:16:17Z", "updated_at": "2016-09-30T08:16:17Z", "author_association": "MEMBER", "body": "Implemented \ud83c\udf89 \n\nNeed to update other modules to reflect the change, and do testing.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/250826415", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-250826415", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 250826415, "node_id": "MDEyOklzc3VlQ29tbWVudDI1MDgyNjQxNQ==", "user": {"login": "honnibal", "id": 8059750, "node_id": "MDQ6VXNlcjgwNTk3NTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8059750?v=4", "gravatar_id": "", "url": "https://api.github.com/users/honnibal", "html_url": "https://github.com/honnibal", "followers_url": "https://api.github.com/users/honnibal/followers", "following_url": "https://api.github.com/users/honnibal/following{/other_user}", "gists_url": "https://api.github.com/users/honnibal/gists{/gist_id}", "starred_url": "https://api.github.com/users/honnibal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/honnibal/subscriptions", "organizations_url": "https://api.github.com/users/honnibal/orgs", "repos_url": "https://api.github.com/users/honnibal/repos", "events_url": "https://api.github.com/users/honnibal/events{/privacy}", "received_events_url": "https://api.github.com/users/honnibal/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-30T19:02:28Z", "updated_at": "2016-09-30T19:02:28Z", "author_association": "MEMBER", "body": "Hmm. I don't want to rush this, because it touches a lot of files, but I also don't want to block the v1.0.0 release, which is otherwise ready. So unfortunately I have to move this out of the milestone. I'll probably get back to it week after next.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/255721389", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-255721389", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 255721389, "node_id": "MDEyOklzc3VlQ29tbWVudDI1NTcyMTM4OQ==", "user": {"login": "honnibal", "id": 8059750, "node_id": "MDQ6VXNlcjgwNTk3NTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8059750?v=4", "gravatar_id": "", "url": "https://api.github.com/users/honnibal", "html_url": "https://github.com/honnibal", "followers_url": "https://api.github.com/users/honnibal/followers", "following_url": "https://api.github.com/users/honnibal/following{/other_user}", "gists_url": "https://api.github.com/users/honnibal/gists{/gist_id}", "starred_url": "https://api.github.com/users/honnibal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/honnibal/subscriptions", "organizations_url": "https://api.github.com/users/honnibal/orgs", "repos_url": "https://api.github.com/users/honnibal/repos", "events_url": "https://api.github.com/users/honnibal/events{/privacy}", "received_events_url": "https://api.github.com/users/honnibal/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-24T12:02:10Z", "updated_at": "2016-10-24T12:03:14Z", "author_association": "MEMBER", "body": "New plan \u2014 let's at least get a good workaround in place, where the user will do some manual management of when the strings will be freed. This should be enough to keep you all productive, while we try to plan out a prettier, 'automagical' solution/wrapper around this. The freeze/flush behaviour is off by default, so it shouldn't disrupt anyone. @tomtung \u2014 I think this is the sort of solution you were looking for, since this makes it a bit easier to control things manually.\n\nSummary:\n- New `freeze` keyword argument to `StringStore.__init__`\n- New `.set_frozen(bool)` method on `StringStore`, controlling whether to start handling new strings as OOV\n- New `.flush_oov()` method on `StringStore`, indicating that the current batch of OOV strings should be flushed away, and the memory freed.\n\nExample (untested):\n\n``` python\n\nnlp = spacy.load('en')\nnlp.vocab.strings.set_frozen(True)\nfor doc in nlp.pipe(texts, batch_size=5000, n_threads=2):\n    do_my_stuff(doc)\n    nlp.vocab.strings.flush_oov()\n```\n\n`.flush_oov()` should be super cheap, so don't stress about trying to call it as late as possible. Call it whenever convenient.\n\nThe OOV strings are encoded using the hash of the byte string. This means that you'll get consistent integer encodings between flushings. However, if you're holding an integer ID for an OOV string, and you flush the OOVs and try to decode the integer, you'll get an `IndexError`. Hopefully, this is logical.\n\nI've pushed the solution to the branch `issue285`. Since the patch is fully backwards compatible, I should be able to push it to PyPi later today \u2014 I just wanted to make sure everything is looking okay, and get some feedback.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/257063861", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-257063861", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 257063861, "node_id": "MDEyOklzc3VlQ29tbWVudDI1NzA2Mzg2MQ==", "user": {"login": "alldefector", "id": 5607717, "node_id": "MDQ6VXNlcjU2MDc3MTc=", "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alldefector", "html_url": "https://github.com/alldefector", "followers_url": "https://api.github.com/users/alldefector/followers", "following_url": "https://api.github.com/users/alldefector/following{/other_user}", "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}", "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions", "organizations_url": "https://api.github.com/users/alldefector/orgs", "repos_url": "https://api.github.com/users/alldefector/repos", "events_url": "https://api.github.com/users/alldefector/events{/privacy}", "received_events_url": "https://api.github.com/users/alldefector/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-29T01:49:33Z", "updated_at": "2016-10-29T01:49:33Z", "author_association": "CONTRIBUTOR", "body": "> However, if you're holding an integer ID for an OOV string, and you flush the OOVs and try to decode the integer, you'll get an IndexError.\n\nIf one serializes a Doc with an OOV word, the above is bound to happen. Since serialization is the only way to reuse parsing results in a data pipeline and most real-world docs would have OOV words, this problem is pretty critical.\n\nWhen serializing a Doc (with `to_bytes`), would it make sense to include the relevant OOV entries? That way, we can deserialize a Doc with only the standard vocab.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/257065909", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-257065909", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 257065909, "node_id": "MDEyOklzc3VlQ29tbWVudDI1NzA2NTkwOQ==", "user": {"login": "alldefector", "id": 5607717, "node_id": "MDQ6VXNlcjU2MDc3MTc=", "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alldefector", "html_url": "https://github.com/alldefector", "followers_url": "https://api.github.com/users/alldefector/followers", "following_url": "https://api.github.com/users/alldefector/following{/other_user}", "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}", "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions", "organizations_url": "https://api.github.com/users/alldefector/orgs", "repos_url": "https://api.github.com/users/alldefector/repos", "events_url": "https://api.github.com/users/alldefector/events{/privacy}", "received_events_url": "https://api.github.com/users/alldefector/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-29T02:33:00Z", "updated_at": "2016-10-29T02:33:00Z", "author_association": "CONTRIBUTOR", "body": "I'd be happy to try to take a crack at it, but things like the use of `HuffmanCodec` in `Packer` make it pretty involved... (actually, given a doc, are we guaranteed to get the same packing result if the vocab grows?)\n\nPerhaps offer a way for self-contained serialization that doesn't depend on any vocab altogether? (Or only depending on a small set of symbols that are future proof)\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/257065988", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-257065988", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 257065988, "node_id": "MDEyOklzc3VlQ29tbWVudDI1NzA2NTk4OA==", "user": {"login": "honnibal", "id": 8059750, "node_id": "MDQ6VXNlcjgwNTk3NTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8059750?v=4", "gravatar_id": "", "url": "https://api.github.com/users/honnibal", "html_url": "https://github.com/honnibal", "followers_url": "https://api.github.com/users/honnibal/followers", "following_url": "https://api.github.com/users/honnibal/following{/other_user}", "gists_url": "https://api.github.com/users/honnibal/gists{/gist_id}", "starred_url": "https://api.github.com/users/honnibal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/honnibal/subscriptions", "organizations_url": "https://api.github.com/users/honnibal/orgs", "repos_url": "https://api.github.com/users/honnibal/repos", "events_url": "https://api.github.com/users/honnibal/events{/privacy}", "received_events_url": "https://api.github.com/users/honnibal/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-29T02:35:28Z", "updated_at": "2016-10-29T02:35:28Z", "author_association": "MEMBER", "body": "The serialiser backs off to a character codec for OOV words.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/257066202", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-257066202", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 257066202, "node_id": "MDEyOklzc3VlQ29tbWVudDI1NzA2NjIwMg==", "user": {"login": "honnibal", "id": 8059750, "node_id": "MDQ6VXNlcjgwNTk3NTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8059750?v=4", "gravatar_id": "", "url": "https://api.github.com/users/honnibal", "html_url": "https://github.com/honnibal", "followers_url": "https://api.github.com/users/honnibal/followers", "following_url": "https://api.github.com/users/honnibal/following{/other_user}", "gists_url": "https://api.github.com/users/honnibal/gists{/gist_id}", "starred_url": "https://api.github.com/users/honnibal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/honnibal/subscriptions", "organizations_url": "https://api.github.com/users/honnibal/orgs", "repos_url": "https://api.github.com/users/honnibal/repos", "events_url": "https://api.github.com/users/honnibal/events{/privacy}", "received_events_url": "https://api.github.com/users/honnibal/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-29T02:40:20Z", "updated_at": "2016-10-29T02:40:33Z", "author_association": "MEMBER", "body": "Incidentally I have regrets about the serialiser. I think I got carried away...\n\nI don't even remember how much bigger a `(text, numpy_array)` tuple would be. Does anyone want to run a benchmark?\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/262391961", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-262391961", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 262391961, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MjM5MTk2MQ==", "user": {"login": "ELind77", "id": 5109720, "node_id": "MDQ6VXNlcjUxMDk3MjA=", "avatar_url": "https://avatars1.githubusercontent.com/u/5109720?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ELind77", "html_url": "https://github.com/ELind77", "followers_url": "https://api.github.com/users/ELind77/followers", "following_url": "https://api.github.com/users/ELind77/following{/other_user}", "gists_url": "https://api.github.com/users/ELind77/gists{/gist_id}", "starred_url": "https://api.github.com/users/ELind77/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ELind77/subscriptions", "organizations_url": "https://api.github.com/users/ELind77/orgs", "repos_url": "https://api.github.com/users/ELind77/repos", "events_url": "https://api.github.com/users/ELind77/events{/privacy}", "received_events_url": "https://api.github.com/users/ELind77/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-22T23:01:18Z", "updated_at": "2016-11-22T23:01:18Z", "author_association": "NONE", "body": "I'm so glad that this has received so much thought and attention! \r\n\r\n@honnibal could we get an update on the current status of this and your thoughts on how best to proceed?\r\n\r\nYour suggestion of splitting the string store seems most in line with my thoughts on this.  If that is still the way you are thinking of going with this and you're thinking of using multiple string stores for OOV words, I'd also just like to put out there that it might be a good idea to use some kind of data structure for storing them other than an array, especially if there are a lot of them.  If the integer ids of the multiple StringStores are guaranteed to never overlap a BST might be a good candidate, if they can overlap though you might need to go with something a bit different, like a UnionFind.\r\n\r\n-- Eric"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/309137771", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-309137771", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 309137771, "node_id": "MDEyOklzc3VlQ29tbWVudDMwOTEzNzc3MQ==", "user": {"login": "rulai-huajunzeng", "id": 20911758, "node_id": "MDQ6VXNlcjIwOTExNzU4", "avatar_url": "https://avatars0.githubusercontent.com/u/20911758?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rulai-huajunzeng", "html_url": "https://github.com/rulai-huajunzeng", "followers_url": "https://api.github.com/users/rulai-huajunzeng/followers", "following_url": "https://api.github.com/users/rulai-huajunzeng/following{/other_user}", "gists_url": "https://api.github.com/users/rulai-huajunzeng/gists{/gist_id}", "starred_url": "https://api.github.com/users/rulai-huajunzeng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rulai-huajunzeng/subscriptions", "organizations_url": "https://api.github.com/users/rulai-huajunzeng/orgs", "repos_url": "https://api.github.com/users/rulai-huajunzeng/repos", "events_url": "https://api.github.com/users/rulai-huajunzeng/events{/privacy}", "received_events_url": "https://api.github.com/users/rulai-huajunzeng/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-16T21:25:06Z", "updated_at": "2017-06-16T21:25:52Z", "author_association": "NONE", "body": "#589 issue still exists. So the workaround doesn't really work. This is one of the blocking issues for us now. Will a more stable fix be available in next 1.x releases? \r\n\r\nThanks a lot for the work!"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/313584368", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-313584368", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 313584368, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMzU4NDM2OA==", "user": {"login": "ELind77", "id": 5109720, "node_id": "MDQ6VXNlcjUxMDk3MjA=", "avatar_url": "https://avatars1.githubusercontent.com/u/5109720?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ELind77", "html_url": "https://github.com/ELind77", "followers_url": "https://api.github.com/users/ELind77/followers", "following_url": "https://api.github.com/users/ELind77/following{/other_user}", "gists_url": "https://api.github.com/users/ELind77/gists{/gist_id}", "starred_url": "https://api.github.com/users/ELind77/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ELind77/subscriptions", "organizations_url": "https://api.github.com/users/ELind77/orgs", "repos_url": "https://api.github.com/users/ELind77/repos", "events_url": "https://api.github.com/users/ELind77/events{/privacy}", "received_events_url": "https://api.github.com/users/ELind77/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-07T04:31:15Z", "updated_at": "2017-07-07T04:31:15Z", "author_association": "NONE", "body": "Hey,\r\n\r\nI just took a look at the StringStore class in main and saw that some work has been done on this.  I still need to play with is a bit to see how it works but this looks really great.  https://github.com/explosion/spaCy/commits/master/spacy/strings.pyx\r\n\r\nThank you so much @honnibal !\r\n\r\n-- Eric "}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/321795676", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-321795676", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 321795676, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMTc5NTY3Ng==", "user": {"login": "azar923", "id": 9512240, "node_id": "MDQ6VXNlcjk1MTIyNDA=", "avatar_url": "https://avatars3.githubusercontent.com/u/9512240?v=4", "gravatar_id": "", "url": "https://api.github.com/users/azar923", "html_url": "https://github.com/azar923", "followers_url": "https://api.github.com/users/azar923/followers", "following_url": "https://api.github.com/users/azar923/following{/other_user}", "gists_url": "https://api.github.com/users/azar923/gists{/gist_id}", "starred_url": "https://api.github.com/users/azar923/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/azar923/subscriptions", "organizations_url": "https://api.github.com/users/azar923/orgs", "repos_url": "https://api.github.com/users/azar923/repos", "events_url": "https://api.github.com/users/azar923/events{/privacy}", "received_events_url": "https://api.github.com/users/azar923/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-11T12:04:04Z", "updated_at": "2017-08-11T12:04:04Z", "author_association": "NONE", "body": "Hi @honnibal \r\n\r\nFirst of all, thank you for this great tool, we use it as part of NLP in our product. However, our case is very high-load system with streaming data (hundreds of thousands emails per day). And we are experiencing the same problem as was discussed here - growth of StringStore causes tremendous memory growth over time, so it really blocks usage of spaCy without fear of crashing the whole system because of OOM. The only workaround we came up with is to reload nlp object each N processed content items and force garbage collector to free memory of deleted object. However, it seem not always working way - sometimes it frees all the memory, and sometimes not. So my questions are as follows:\r\n1) Is it planned to deal with this issue somehow? From what I see, in version 2.0 the problem still exists.\r\n\r\n2) If it is such a fundamental way how spaCy works, maybe, there are some more clever workarounds to prevent such memory leaks?\r\n\r\nThanks in advance."}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/321809477", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-321809477", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 321809477, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMTgwOTQ3Nw==", "user": {"login": "honnibal", "id": 8059750, "node_id": "MDQ6VXNlcjgwNTk3NTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8059750?v=4", "gravatar_id": "", "url": "https://api.github.com/users/honnibal", "html_url": "https://github.com/honnibal", "followers_url": "https://api.github.com/users/honnibal/followers", "following_url": "https://api.github.com/users/honnibal/following{/other_user}", "gists_url": "https://api.github.com/users/honnibal/gists{/gist_id}", "starred_url": "https://api.github.com/users/honnibal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/honnibal/subscriptions", "organizations_url": "https://api.github.com/users/honnibal/orgs", "repos_url": "https://api.github.com/users/honnibal/repos", "events_url": "https://api.github.com/users/honnibal/events{/privacy}", "received_events_url": "https://api.github.com/users/honnibal/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-11T13:17:16Z", "updated_at": "2017-08-11T13:17:16Z", "author_association": "MEMBER", "body": "@azar923 Did you try the `set_frozen(True)` mitigation above?\r\n\r\nThe situation around this is much improved in spaCy 2, because the string-to-integer mapping no longer depends on the `StringStore` state --- it's just a hash value. This makes everything much easier. First, the `StringStore` is smaller per unit, but more importantly, if you're streaming documents through, we can restore the original string store every N documents without causing any problems."}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/321830786", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-321830786", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 321830786, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMTgzMDc4Ng==", "user": {"login": "azar923", "id": 9512240, "node_id": "MDQ6VXNlcjk1MTIyNDA=", "avatar_url": "https://avatars3.githubusercontent.com/u/9512240?v=4", "gravatar_id": "", "url": "https://api.github.com/users/azar923", "html_url": "https://github.com/azar923", "followers_url": "https://api.github.com/users/azar923/followers", "following_url": "https://api.github.com/users/azar923/following{/other_user}", "gists_url": "https://api.github.com/users/azar923/gists{/gist_id}", "starred_url": "https://api.github.com/users/azar923/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/azar923/subscriptions", "organizations_url": "https://api.github.com/users/azar923/orgs", "repos_url": "https://api.github.com/users/azar923/repos", "events_url": "https://api.github.com/users/azar923/events{/privacy}", "received_events_url": "https://api.github.com/users/azar923/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-11T14:36:21Z", "updated_at": "2017-08-11T14:36:21Z", "author_association": "NONE", "body": "@honnibal thank you for quick answer,\r\nYes, I tried set_frozen(True) but experienced the same issue https://github.com/explosion/spaCy/issues/589\r\nI use 1.6 now, did not try older versions yet because of some performance degradation in one-thread mode, which is critical for us now.\r\n\"we can restore the original string store every N documents without causing any problems\" - sorry, did not catch that, how is it restored every N documents?"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/321846725", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-321846725", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 321846725, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMTg0NjcyNQ==", "user": {"login": "honnibal", "id": 8059750, "node_id": "MDQ6VXNlcjgwNTk3NTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8059750?v=4", "gravatar_id": "", "url": "https://api.github.com/users/honnibal", "html_url": "https://github.com/honnibal", "followers_url": "https://api.github.com/users/honnibal/followers", "following_url": "https://api.github.com/users/honnibal/following{/other_user}", "gists_url": "https://api.github.com/users/honnibal/gists{/gist_id}", "starred_url": "https://api.github.com/users/honnibal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/honnibal/subscriptions", "organizations_url": "https://api.github.com/users/honnibal/orgs", "repos_url": "https://api.github.com/users/honnibal/repos", "events_url": "https://api.github.com/users/honnibal/events{/privacy}", "received_events_url": "https://api.github.com/users/honnibal/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-11T15:38:40Z", "updated_at": "2017-08-11T15:38:40Z", "author_association": "MEMBER", "body": "The restoration idea would look like this:\r\n\r\n```python\r\n\r\n    backup_strings_data = nlp.vocab.strings.to_bytes()\r\n    backup_strings = StringStore().from_bytes(backup_strings_data)\r\n    for i, doc in enumerate(nlp.pipe):\r\n        yield doc\r\n        for word in doc:\r\n            backup_strings.add(word.text)\r\n        if i % 1000 == 999:\r\n            nlp.vocab.strings = backup_strings\r\n            backup_strings = StringStore().from_bytes(backup_strings_data)\r\n```\r\n\r\nThis would ensure that strings stay available from only the last 1000 documents. It works by keeping two copies of the `StringStore`: the active one, and the backup. The backup tracks the active store for 1000 documents, and then takes over. We then start a new backup from the original strings data, which adds entries for the next 1000 documents, so that when it takes over, those recent documents' strings will be available."}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/321861935", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-321861935", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 321861935, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMTg2MTkzNQ==", "user": {"login": "azar923", "id": 9512240, "node_id": "MDQ6VXNlcjk1MTIyNDA=", "avatar_url": "https://avatars3.githubusercontent.com/u/9512240?v=4", "gravatar_id": "", "url": "https://api.github.com/users/azar923", "html_url": "https://github.com/azar923", "followers_url": "https://api.github.com/users/azar923/followers", "following_url": "https://api.github.com/users/azar923/following{/other_user}", "gists_url": "https://api.github.com/users/azar923/gists{/gist_id}", "starred_url": "https://api.github.com/users/azar923/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/azar923/subscriptions", "organizations_url": "https://api.github.com/users/azar923/orgs", "repos_url": "https://api.github.com/users/azar923/repos", "events_url": "https://api.github.com/users/azar923/events{/privacy}", "received_events_url": "https://api.github.com/users/azar923/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-11T16:42:43Z", "updated_at": "2017-08-11T16:42:43Z", "author_association": "NONE", "body": "@honnibal Great, thanks very much for these improvements. Will look forward to 2.0 release to try. For now workaround with reloading / collecting nlp object works quite ok in production."}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/326172744", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-326172744", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 326172744, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNjE3Mjc0NA==", "user": {"login": "vmandke", "id": 3072795, "node_id": "MDQ6VXNlcjMwNzI3OTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/3072795?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vmandke", "html_url": "https://github.com/vmandke", "followers_url": "https://api.github.com/users/vmandke/followers", "following_url": "https://api.github.com/users/vmandke/following{/other_user}", "gists_url": "https://api.github.com/users/vmandke/gists{/gist_id}", "starred_url": "https://api.github.com/users/vmandke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vmandke/subscriptions", "organizations_url": "https://api.github.com/users/vmandke/orgs", "repos_url": "https://api.github.com/users/vmandke/repos", "events_url": "https://api.github.com/users/vmandke/events{/privacy}", "received_events_url": "https://api.github.com/users/vmandke/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-31T02:36:30Z", "updated_at": "2017-08-31T08:18:53Z", "author_association": "NONE", "body": "@honnibal  I'm also facing the same issue, (spacy 1.5.0). I know this is hackish, however, would resetting the _map and setting size to 0, or resetting the StringStore itself after a certain critical size is reached could cause any problems? Currently using spacy to get the POS tags. (from sentence subtree etc)\r\nIs there a way to reset the StringStore without reloading the model again ? The workaround of using set_frozen does not work.\r\nRef: [v1.5.0_source](https://github.com/explosion/spaCy/blob/v1.5.0/spacy/strings.pyx)"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/326921230", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-326921230", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 326921230, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNjkyMTIzMA==", "user": {"login": "oroszgy", "id": 145334, "node_id": "MDQ6VXNlcjE0NTMzNA==", "avatar_url": "https://avatars2.githubusercontent.com/u/145334?v=4", "gravatar_id": "", "url": "https://api.github.com/users/oroszgy", "html_url": "https://github.com/oroszgy", "followers_url": "https://api.github.com/users/oroszgy/followers", "following_url": "https://api.github.com/users/oroszgy/following{/other_user}", "gists_url": "https://api.github.com/users/oroszgy/gists{/gist_id}", "starred_url": "https://api.github.com/users/oroszgy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/oroszgy/subscriptions", "organizations_url": "https://api.github.com/users/oroszgy/orgs", "repos_url": "https://api.github.com/users/oroszgy/repos", "events_url": "https://api.github.com/users/oroszgy/events{/privacy}", "received_events_url": "https://api.github.com/users/oroszgy/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-04T09:58:50Z", "updated_at": "2017-09-04T09:58:50Z", "author_association": "CONTRIBUTOR", "body": "I am experimenting with this workaround with the 1.x version. So far it is working well.\r\n\r\n```python\r\nfrom concurrent.futures import ThreadPoolExecutor\r\n\r\nclass RestartingEnglish:\r\n    RESTART_CALLS = 1000000\r\n    \r\n    def __init__(self, *args, **kwargs):\r\n        self.nr_calls = 0\r\n        self._init_args = args\r\n        self._init_kwargs = kwargs\r\n        self.nlp = self._init_nlp()\r\n        self.fut_nlp = None\r\n        self.exec = ThreadPoolExecutor(max_workers=1)\r\n        \r\n    def _init_nlp(self):\r\n        return English(*self._init_args, **self._init_kwargs)\r\n        \r\n    def _restart_nlp(self):\r\n        self.nr_calls += 1\r\n        if self.nr_calls >= self.RESTART_CALLS:\r\n            if self.fut_nlp == None:\r\n                print(\"Getting new NLP\", self.nr_calls)\r\n                self.fut_nlp = self.exec.submit(self._init_nlp)\r\n            else:\r\n                if self.fut_nlp.done():\r\n                    print(\"Got new NLP\", self.nr_calls)\r\n                    self.nlp = self.fut_nlp.result()\r\n                    self.fut_nlp = None\r\n                    self.nr_calls = 0\r\n\r\n    def __call__(self, *args, **kwargs):\r\n        doc = self.nlp(*args, **kwargs)\r\n        self._restart_nlp()\r\n        return doc\r\n```"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/336981797", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-336981797", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 336981797, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNjk4MTc5Nw==", "user": {"login": "honnibal", "id": 8059750, "node_id": "MDQ6VXNlcjgwNTk3NTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8059750?v=4", "gravatar_id": "", "url": "https://api.github.com/users/honnibal", "html_url": "https://github.com/honnibal", "followers_url": "https://api.github.com/users/honnibal/followers", "following_url": "https://api.github.com/users/honnibal/following{/other_user}", "gists_url": "https://api.github.com/users/honnibal/gists{/gist_id}", "starred_url": "https://api.github.com/users/honnibal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/honnibal/subscriptions", "organizations_url": "https://api.github.com/users/honnibal/orgs", "repos_url": "https://api.github.com/users/honnibal/repos", "events_url": "https://api.github.com/users/honnibal/events{/privacy}", "received_events_url": "https://api.github.com/users/honnibal/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-16T18:20:36Z", "updated_at": "2017-10-16T18:20:36Z", "author_association": "MEMBER", "body": "Fixed! (!!!!)\r\n:tada: :tada: :tada:\r\n\r\nPlease see https://github.com/explosion/spaCy/pull/1424"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/387421283", "html_url": "https://github.com/explosion/spaCy/issues/285#issuecomment-387421283", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/285", "id": 387421283, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NzQyMTI4Mw==", "user": {"login": "lock[bot]", "id": 33595554, "node_id": "MDM6Qm90MzM1OTU1NTQ=", "avatar_url": "https://avatars1.githubusercontent.com/in/6672?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lock%5Bbot%5D", "html_url": "https://github.com/apps/lock", "followers_url": "https://api.github.com/users/lock%5Bbot%5D/followers", "following_url": "https://api.github.com/users/lock%5Bbot%5D/following{/other_user}", "gists_url": "https://api.github.com/users/lock%5Bbot%5D/gists{/gist_id}", "starred_url": "https://api.github.com/users/lock%5Bbot%5D/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lock%5Bbot%5D/subscriptions", "organizations_url": "https://api.github.com/users/lock%5Bbot%5D/orgs", "repos_url": "https://api.github.com/users/lock%5Bbot%5D/repos", "events_url": "https://api.github.com/users/lock%5Bbot%5D/events{/privacy}", "received_events_url": "https://api.github.com/users/lock%5Bbot%5D/received_events", "type": "Bot", "site_admin": false}, "created_at": "2018-05-08T14:27:37Z", "updated_at": "2018-05-08T14:27:37Z", "author_association": "NONE", "body": "This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.\n"}]