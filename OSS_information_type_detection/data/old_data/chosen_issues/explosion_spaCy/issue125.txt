[{
  "url": "https://api.github.com/repos/explosion/spaCy/issues/125",
  "repository_url": "https://api.github.com/repos/explosion/spaCy",
  "labels_url": "https://api.github.com/repos/explosion/spaCy/issues/125/labels{/name}",
  "comments_url": "https://api.github.com/repos/explosion/spaCy/issues/125/comments",
  "events_url": "https://api.github.com/repos/explosion/spaCy/issues/125/events",
  "html_url": "https://github.com/explosion/spaCy/issues/125",
  "id": 109689721,
  "node_id": "MDU6SXNzdWUxMDk2ODk3MjE=",
  "number": 125,
  "title": "Use in Apache Spark / English() object cannot be pickled",
  "user": {
    "login": "aeneaswiener",
    "id": 4281507,
    "node_id": "MDQ6VXNlcjQyODE1MDc=",
    "avatar_url": "https://avatars1.githubusercontent.com/u/4281507?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/aeneaswiener",
    "html_url": "https://github.com/aeneaswiener",
    "followers_url": "https://api.github.com/users/aeneaswiener/followers",
    "following_url": "https://api.github.com/users/aeneaswiener/following{/other_user}",
    "gists_url": "https://api.github.com/users/aeneaswiener/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/aeneaswiener/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/aeneaswiener/subscriptions",
    "organizations_url": "https://api.github.com/users/aeneaswiener/orgs",
    "repos_url": "https://api.github.com/users/aeneaswiener/repos",
    "events_url": "https://api.github.com/users/aeneaswiener/events{/privacy}",
    "received_events_url": "https://api.github.com/users/aeneaswiener/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [
    {
      "id": 111380487,
      "node_id": "MDU6TGFiZWwxMTEzODA0ODc=",
      "url": "https://api.github.com/repos/explosion/spaCy/labels/enhancement",
      "name": "enhancement",
      "color": "20834E",
      "default": true
    },
    {
      "id": 496348994,
      "node_id": "MDU6TGFiZWw0OTYzNDg5OTQ=",
      "url": "https://api.github.com/repos/explosion/spaCy/labels/%F0%9F%8C%99%20nightly",
      "name": "ðŸŒ™ nightly",
      "color": "1a1e23",
      "default": false
    }
  ],
  "state": "closed",
  "locked": true,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 53,
  "created_at": "2015-10-04T16:22:26Z",
  "updated_at": "2018-05-08T06:55:50Z",
  "closed_at": "2017-05-07T16:21:20Z",
  "author_association": "NONE",
  "body": "For spaCy to work out of the box with [Apache Spark](http://spark.apache.org) the language modles need to be pickled so that they can be initialised on the master node and then sent to the workers. \n\nThis currently doesn't work with plain pickle, failing as follows:\n\n```\n>>> from __future__ import unicode_literals, print_function\n>>> from spacy.en import English\n>>> import pickle\n>>> nlp = English()\n>>> nlpp = pickle.dumps(nlp)\nTraceback (most recent call last):\n[...]\nTypeError: can't pickle Vocab objects\n```\n\nApache Spark ships with a package called [cloudpickle](https://github.com/cloudpipe/cloudpickle) which is meant to support a wider set of Python constructs, but serialisation with cloudpickle also fails resulting in a segmentation fault:\n\n```\n>>> from pyspark import cloudpickle\n>>> pickled_nlp = cloudpickle.dumps(nlp)\n>>> nlpp = pickle.dumps(nlp)\n>>> nlpp('test text')\nSegmentation fault\n```\n\nBy default Apache Spark uses pickle, but can be told to use cloudpickle instead.\n\nCurrently a feasable workaround is lazy loading of the language models on the worker nodes:\n\n```\nglobal nlp\ndef lazyloaded_nlp(s):\n    global nlp\n    try:\n        return nlp(s)\n    except:\n        nlp = English()\n        return nlp(s)\n```\n\nThe above works. Nevertheless, I wonder if it would be possible to make the English() object pickleable? If not too difficult from your end, having the language models pickleable would provide a better out of box experience for Apache Spark users.\n",
  "closed_by": {
    "login": "ines",
    "id": 13643239,
    "node_id": "MDQ6VXNlcjEzNjQzMjM5",
    "avatar_url": "https://avatars0.githubusercontent.com/u/13643239?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/ines",
    "html_url": "https://github.com/ines",
    "followers_url": "https://api.github.com/users/ines/followers",
    "following_url": "https://api.github.com/users/ines/following{/other_user}",
    "gists_url": "https://api.github.com/users/ines/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/ines/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/ines/subscriptions",
    "organizations_url": "https://api.github.com/users/ines/orgs",
    "repos_url": "https://api.github.com/users/ines/repos",
    "events_url": "https://api.github.com/users/ines/events{/privacy}",
    "received_events_url": "https://api.github.com/users/ines/received_events",
    "type": "User",
    "site_admin": false
  }
},{"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/145836952", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-145836952", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 145836952, "node_id": "MDEyOklzc3VlQ29tbWVudDE0NTgzNjk1Mg==", "user": {"login": "honnibal", "id": 8059750, "node_id": "MDQ6VXNlcjgwNTk3NTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8059750?v=4", "gravatar_id": "", "url": "https://api.github.com/users/honnibal", "html_url": "https://github.com/honnibal", "followers_url": "https://api.github.com/users/honnibal/followers", "following_url": "https://api.github.com/users/honnibal/following{/other_user}", "gists_url": "https://api.github.com/users/honnibal/gists{/gist_id}", "starred_url": "https://api.github.com/users/honnibal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/honnibal/subscriptions", "organizations_url": "https://api.github.com/users/honnibal/orgs", "repos_url": "https://api.github.com/users/honnibal/repos", "events_url": "https://api.github.com/users/honnibal/events{/privacy}", "received_events_url": "https://api.github.com/users/honnibal/received_events", "type": "User", "site_admin": false}, "created_at": "2015-10-06T12:03:19Z", "updated_at": "2015-10-06T12:03:19Z", "author_association": "MEMBER", "body": "I've spent a little time looking into this now.\n\nThe workflow that's a little bit tricky to support is something like this:\n- Create an `English()` instance\n- Change the state of some binary data, e.g. modify the lexicon\n- Send it to workers, with new state preserved\n\nNow, when I say \"a little bit tricky\"...If this is a requirement, we can do it. It'll mean writing out all the state to binary data strings, shipping ~1gb to each worker, and then loading from the strings. The patch will touch every class, and it might be fiddly, especially to keep efficiency nice. But there's no real problem.\n\nThe question is whether this work-flow is really important. I would've thought that the better way to do things was to divide the documents in the master node, and then send a reference to a function like this:\n\n``` python\n\ndef do_work(batch_of_texts):\n    nlp = English()\n    for text in texts:\n        doc = nlp(text)\n        # Stuff\n\ndistribute(texts, do_work, n_workers=10)\n```\n\nDoes PySpark not work this way?\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/145906045", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-145906045", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 145906045, "node_id": "MDEyOklzc3VlQ29tbWVudDE0NTkwNjA0NQ==", "user": {"login": "aeneaswiener", "id": 4281507, "node_id": "MDQ6VXNlcjQyODE1MDc=", "avatar_url": "https://avatars1.githubusercontent.com/u/4281507?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aeneaswiener", "html_url": "https://github.com/aeneaswiener", "followers_url": "https://api.github.com/users/aeneaswiener/followers", "following_url": "https://api.github.com/users/aeneaswiener/following{/other_user}", "gists_url": "https://api.github.com/users/aeneaswiener/gists{/gist_id}", "starred_url": "https://api.github.com/users/aeneaswiener/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aeneaswiener/subscriptions", "organizations_url": "https://api.github.com/users/aeneaswiener/orgs", "repos_url": "https://api.github.com/users/aeneaswiener/repos", "events_url": "https://api.github.com/users/aeneaswiener/events{/privacy}", "received_events_url": "https://api.github.com/users/aeneaswiener/received_events", "type": "User", "site_admin": false}, "created_at": "2015-10-06T15:46:04Z", "updated_at": "2015-10-06T15:47:41Z", "author_association": "NONE", "body": "Yes the way you suggest is similar to what I am doing now with a lazy loaded `English()` object. I mainly created this issue in case someone else is running into similar problems when trying spaCy on Spark, as the error messages raised by Spark when failing to pickle the language models are not very helpful (workers just crash because of segmentation fault). This issue can be closed from my end.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/145907341", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-145907341", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 145907341, "node_id": "MDEyOklzc3VlQ29tbWVudDE0NTkwNzM0MQ==", "user": {"login": "honnibal", "id": 8059750, "node_id": "MDQ6VXNlcjgwNTk3NTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8059750?v=4", "gravatar_id": "", "url": "https://api.github.com/users/honnibal", "html_url": "https://github.com/honnibal", "followers_url": "https://api.github.com/users/honnibal/followers", "following_url": "https://api.github.com/users/honnibal/following{/other_user}", "gists_url": "https://api.github.com/users/honnibal/gists{/gist_id}", "starred_url": "https://api.github.com/users/honnibal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/honnibal/subscriptions", "organizations_url": "https://api.github.com/users/honnibal/orgs", "repos_url": "https://api.github.com/users/honnibal/repos", "events_url": "https://api.github.com/users/honnibal/events{/privacy}", "received_events_url": "https://api.github.com/users/honnibal/received_events", "type": "User", "site_admin": false}, "created_at": "2015-10-06T15:51:01Z", "updated_at": "2015-10-06T15:51:01Z", "author_association": "MEMBER", "body": "Thanks.\n\nLet's leave it open for now and think about it a bit more.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/146385943", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-146385943", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 146385943, "node_id": "MDEyOklzc3VlQ29tbWVudDE0NjM4NTk0Mw==", "user": {"login": "chrisdubois", "id": 1958804, "node_id": "MDQ6VXNlcjE5NTg4MDQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/1958804?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chrisdubois", "html_url": "https://github.com/chrisdubois", "followers_url": "https://api.github.com/users/chrisdubois/followers", "following_url": "https://api.github.com/users/chrisdubois/following{/other_user}", "gists_url": "https://api.github.com/users/chrisdubois/gists{/gist_id}", "starred_url": "https://api.github.com/users/chrisdubois/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chrisdubois/subscriptions", "organizations_url": "https://api.github.com/users/chrisdubois/orgs", "repos_url": "https://api.github.com/users/chrisdubois/repos", "events_url": "https://api.github.com/users/chrisdubois/events{/privacy}", "received_events_url": "https://api.github.com/users/chrisdubois/received_events", "type": "User", "site_admin": false}, "created_at": "2015-10-08T01:02:29Z", "updated_at": "2015-10-08T01:02:29Z", "author_association": "CONTRIBUTOR", "body": "@honnibal When you say \"writing out all the state\", can you clarify what is involved here? Which objects inside of `nlp`'s state are easy to serialize and which ones aren't?\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/146388747", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-146388747", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 146388747, "node_id": "MDEyOklzc3VlQ29tbWVudDE0NjM4ODc0Nw==", "user": {"login": "honnibal", "id": 8059750, "node_id": "MDQ6VXNlcjgwNTk3NTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8059750?v=4", "gravatar_id": "", "url": "https://api.github.com/users/honnibal", "html_url": "https://github.com/honnibal", "followers_url": "https://api.github.com/users/honnibal/followers", "following_url": "https://api.github.com/users/honnibal/following{/other_user}", "gists_url": "https://api.github.com/users/honnibal/gists{/gist_id}", "starred_url": "https://api.github.com/users/honnibal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/honnibal/subscriptions", "organizations_url": "https://api.github.com/users/honnibal/orgs", "repos_url": "https://api.github.com/users/honnibal/repos", "events_url": "https://api.github.com/users/honnibal/events{/privacy}", "received_events_url": "https://api.github.com/users/honnibal/received_events", "type": "User", "site_admin": false}, "created_at": "2015-10-08T01:29:00Z", "updated_at": "2015-10-08T01:52:49Z", "author_association": "MEMBER", "body": "Good question. A quick summary:\n\n| Class | Instance | Changes if | Frequently changes? | Size | Description |\n| --- | --- | --- | --- | --- | --- |\n| `StringStore` | `nlp.vocab.strings` | Any unseen token is processed | Yes | 9mb | Mapping table of strings to integers. Serialized as list of strings. |\n| `LexemeC*` | `nlp.vocab._by_orth` | User writes Lexeme properties | Maybe | 80mb | Pre-computed attributes for every lexical type in the vocabulary. |\n| `numpy.ndarray` | `nlp.vocab.vectors` | User updates word vectors | Maybe | 200mb to 1gb | Word vectors. |\n| `Matcher` | `nlp.matcher` | User adds to gazetteer | Yes | <1mb | User-custom entity recognition behaviour. |\n| `Packer` | `nlp.vocab.serializer` | Shouldn't, but must check. | Think no? | Small | I think the serializer reads data off the vocab. But need to double check. |\n| `thinc.LinearModel` | `nlp.parser.model` | Training | No | 490mb | Parser's statistical model. Immutable in normal use. |\n| `thinc.LinearModel` | `nlp.entity.model` | Training | No | 38mb | NER statistical model. Immutable in normal use. |\n| `thinc.LinearModel` | `nlp.tagger.model` | Training | No | 12mb | POS statistical model. Immutable in normal use. |\n\nReally everything could be serialized, because everything knows how to write itself to disk and load itself back, usually in a binary format. One super simple way to do the serialization would be to dump to a directory, tar it, send the bits, untar, load. I'm sure we can do better than that, though,\n\nIn standard use, the only things likely to change are within `Vocab`.\n\nIf nothing is written to the NLP class, then serialization with Pickle is super simple: we can just tell it to load with the arguments originally passed to the constructor.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/147018457", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-147018457", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 147018457, "node_id": "MDEyOklzc3VlQ29tbWVudDE0NzAxODQ1Nw==", "user": {"login": "jkbradley", "id": 5084283, "node_id": "MDQ6VXNlcjUwODQyODM=", "avatar_url": "https://avatars3.githubusercontent.com/u/5084283?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jkbradley", "html_url": "https://github.com/jkbradley", "followers_url": "https://api.github.com/users/jkbradley/followers", "following_url": "https://api.github.com/users/jkbradley/following{/other_user}", "gists_url": "https://api.github.com/users/jkbradley/gists{/gist_id}", "starred_url": "https://api.github.com/users/jkbradley/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jkbradley/subscriptions", "organizations_url": "https://api.github.com/users/jkbradley/orgs", "repos_url": "https://api.github.com/users/jkbradley/repos", "events_url": "https://api.github.com/users/jkbradley/events{/privacy}", "received_events_url": "https://api.github.com/users/jkbradley/received_events", "type": "User", "site_admin": false}, "created_at": "2015-10-10T00:56:51Z", "updated_at": "2015-10-10T00:56:51Z", "author_association": "NONE", "body": "Just to chime in: Based on my experience with Spark, it sounds like loading English locally at each worker is best.  Does this summary of pros/cons sound about right?\n\nPros\n- Less communication (so faster)\n  - If the English object will be used on workers multiple times, it'd be important to cache it to prevent re-loading.\n- Simpler to implement\n  - This seems like the biggest argument---the cost of dev time.  Also, serialization might have to be updated if the English object gets updated.\n- Less memory used on driver\n  - It's much better to do heavy work on workers than on the driver.  If an executor dies, things are often recoverable, but more may be lost when the driver dies.\n\nCons\n- Need to ensure configuration/setup of English is identical across workers\n  - This was the part I was not sure about: Is there any setup required, or is it sufficient to just load the English object at each worker?  If there is some setup, then it'd be worth discussing what needs to be communicated for each worker to do the same setup.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/147019524", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-147019524", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 147019524, "node_id": "MDEyOklzc3VlQ29tbWVudDE0NzAxOTUyNA==", "user": {"login": "honnibal", "id": 8059750, "node_id": "MDQ6VXNlcjgwNTk3NTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8059750?v=4", "gravatar_id": "", "url": "https://api.github.com/users/honnibal", "html_url": "https://github.com/honnibal", "followers_url": "https://api.github.com/users/honnibal/followers", "following_url": "https://api.github.com/users/honnibal/following{/other_user}", "gists_url": "https://api.github.com/users/honnibal/gists{/gist_id}", "starred_url": "https://api.github.com/users/honnibal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/honnibal/subscriptions", "organizations_url": "https://api.github.com/users/honnibal/orgs", "repos_url": "https://api.github.com/users/honnibal/repos", "events_url": "https://api.github.com/users/honnibal/events{/privacy}", "received_events_url": "https://api.github.com/users/honnibal/received_events", "type": "User", "site_admin": false}, "created_at": "2015-10-10T01:08:53Z", "updated_at": "2015-10-10T01:12:01Z", "author_association": "MEMBER", "body": "Thanks.\n\nOkay so. To be clear, the **wrong** way to do this is to batch the texts into jobs yourself, and then map over the jobs, right? Instead if the task is per text, we should let Spark map over the texts, and it's our/user's job to make that work.\n\nIt's also wrong to let Spark see the English instance as a shared variable. It needs to be private to the workers, because we really don't want the workers to transfer it back to the driver.\n\nIn terms of the trade-offs, my perspective is that we're happy to take on implementation complexity. The library is very willing to do work so that users don't have to.\n\nThey key priority is that the library should make the right thing easy. Whatever our recommended workflow is, it should be the obvious and least-effort thing to do. But if there's an important trade-off to make, we don't want to have a silent default that just picks an option, and then the user gets a bad result and has to go back and figure out what went wrong.\n\nI worry that adding this pickling capability will lead users down the wrong track. It seems to me that it makes the whole thing transparent, when actually there's a meaningful decision here, that maybe the user needs to make. Masking that decision isn't necessarily a good service to them.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/147120516", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-147120516", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 147120516, "node_id": "MDEyOklzc3VlQ29tbWVudDE0NzEyMDUxNg==", "user": {"login": "jkbradley", "id": 5084283, "node_id": "MDQ6VXNlcjUwODQyODM=", "avatar_url": "https://avatars3.githubusercontent.com/u/5084283?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jkbradley", "html_url": "https://github.com/jkbradley", "followers_url": "https://api.github.com/users/jkbradley/followers", "following_url": "https://api.github.com/users/jkbradley/following{/other_user}", "gists_url": "https://api.github.com/users/jkbradley/gists{/gist_id}", "starred_url": "https://api.github.com/users/jkbradley/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jkbradley/subscriptions", "organizations_url": "https://api.github.com/users/jkbradley/orgs", "repos_url": "https://api.github.com/users/jkbradley/repos", "events_url": "https://api.github.com/users/jkbradley/events{/privacy}", "received_events_url": "https://api.github.com/users/jkbradley/received_events", "type": "User", "site_admin": false}, "created_at": "2015-10-10T19:50:29Z", "updated_at": "2015-10-10T19:52:34Z", "author_association": "NONE", "body": "> To be clear, the wrong way to do this is to batch the texts into jobs yourself, and then map over the jobs, right? Instead if the task is per text, we should let Spark map over the texts, and it's our/user's job to make that work.\n\nIf it's a pure map operation, then it is better to let Spark handle the map, if only because it can then handle distributing the work and providing some fault tolerance.\n\n> It's also wrong to let Spark see the English instance as a shared variable. It needs to be private to the workers, because we really don't want the workers to transfer it back to the driver.\n\nIt's not wrong, but it is not ideal since 1GB is a fairly big object.  It'd be nice to design an API which encouraged minimal communication: either (a) load it on the driver and broadcast it (transfer only once) or (b) load it on each worker (once).  The API could potentially handle this under the hood.\n\n# \n\nIf you're not worried about development effort, then actually serialization sounds like the best option.  But you could hide it from the user.  Here's what I'm thinking:\n- Goal: Have users use the same English object on the driver or in Spark jobs, and not worry about communicating the big object.\n- Implementation A: Simpler, but requires user to be aware of local vs. distributed operations\n  - When the user wants to do a distributed operation, they call `nlp(myRDDofTexts)` on the driver, and English handles the map under the hood.\n  - Under the hood, English could broadcast itself (requiring pickling), or just load itself locally on each worker.\n- Implementation B: More complex, but user can be oblivious\n  - Within English, make the big objects transient (i.e., specify that via `__getstate__` for pickling).\n  - Store those big objects in broadcast variables (initialized on the driver in the constructor), which are then read when needed on the workers.\n\nBoth of these options require pickling, but B requires somewhat more complex pickling.\n\nAfter thinking more about it, I'd recommend broadcasting English (unless you think it will grow far beyond 1GB in the future).  That will be easier for handling English's settings or parameters.\n\n# \n\n> I worry that adding this pickling capability will lead users down the wrong track.\n\nI think it's OK to hide things from the user.  As far as I can tell, the main things the user needs to be aware of are (a) whether any options they set will be used both on the driver and on workers and (b) when a data object is local vs. distributed.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/147142713", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-147142713", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 147142713, "node_id": "MDEyOklzc3VlQ29tbWVudDE0NzE0MjcxMw==", "user": {"login": "honnibal", "id": 8059750, "node_id": "MDQ6VXNlcjgwNTk3NTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8059750?v=4", "gravatar_id": "", "url": "https://api.github.com/users/honnibal", "html_url": "https://github.com/honnibal", "followers_url": "https://api.github.com/users/honnibal/followers", "following_url": "https://api.github.com/users/honnibal/following{/other_user}", "gists_url": "https://api.github.com/users/honnibal/gists{/gist_id}", "starred_url": "https://api.github.com/users/honnibal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/honnibal/subscriptions", "organizations_url": "https://api.github.com/users/honnibal/orgs", "repos_url": "https://api.github.com/users/honnibal/repos", "events_url": "https://api.github.com/users/honnibal/events{/privacy}", "received_events_url": "https://api.github.com/users/honnibal/received_events", "type": "User", "site_admin": false}, "created_at": "2015-10-11T01:07:18Z", "updated_at": "2015-10-11T01:07:18Z", "author_association": "MEMBER", "body": "Okay, thanks. Is this how it works on sk-learn, Theano, etc?\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/147295974", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-147295974", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 147295974, "node_id": "MDEyOklzc3VlQ29tbWVudDE0NzI5NTk3NA==", "user": {"login": "honnibal", "id": 8059750, "node_id": "MDQ6VXNlcjgwNTk3NTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8059750?v=4", "gravatar_id": "", "url": "https://api.github.com/users/honnibal", "html_url": "https://github.com/honnibal", "followers_url": "https://api.github.com/users/honnibal/followers", "following_url": "https://api.github.com/users/honnibal/following{/other_user}", "gists_url": "https://api.github.com/users/honnibal/gists{/gist_id}", "starred_url": "https://api.github.com/users/honnibal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/honnibal/subscriptions", "organizations_url": "https://api.github.com/users/honnibal/orgs", "repos_url": "https://api.github.com/users/honnibal/repos", "events_url": "https://api.github.com/users/honnibal/events{/privacy}", "received_events_url": "https://api.github.com/users/honnibal/received_events", "type": "User", "site_admin": false}, "created_at": "2015-10-12T05:45:52Z", "updated_at": "2015-10-12T05:45:52Z", "author_association": "MEMBER", "body": "Can I expect tempfiles and tempdirs to work? Is the file system shared between the driver and all children?\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/147335821", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-147335821", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 147335821, "node_id": "MDEyOklzc3VlQ29tbWVudDE0NzMzNTgyMQ==", "user": {"login": "honnibal", "id": 8059750, "node_id": "MDQ6VXNlcjgwNTk3NTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8059750?v=4", "gravatar_id": "", "url": "https://api.github.com/users/honnibal", "html_url": "https://github.com/honnibal", "followers_url": "https://api.github.com/users/honnibal/followers", "following_url": "https://api.github.com/users/honnibal/following{/other_user}", "gists_url": "https://api.github.com/users/honnibal/gists{/gist_id}", "starred_url": "https://api.github.com/users/honnibal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/honnibal/subscriptions", "organizations_url": "https://api.github.com/users/honnibal/orgs", "repos_url": "https://api.github.com/users/honnibal/repos", "events_url": "https://api.github.com/users/honnibal/events{/privacy}", "received_events_url": "https://api.github.com/users/honnibal/received_events", "type": "User", "site_admin": false}, "created_at": "2015-10-12T09:01:43Z", "updated_at": "2015-10-12T09:03:34Z", "author_association": "MEMBER", "body": "Okay so `cloudpickle.dump(nlp, file_)` is working, in a branch (`attrs`). I'll do some further testing with Spark before pushing it up to master.\n\nI'm concerned that 100% correctness seems difficult to achieve here. The only _known_ problem with the current implementation arises when you call the `.train()` methods prior to pickling. But, I haven't done much to test against various subtle problems that might occur. There's a lot of potential state, and guaranteeing that 100% of it is saved is difficult. The current implementation also saves to disk, using tempfiles. This might prove to be problematic.\n\nOnce we're playing with the current implementation, we can see whether another approach might be better.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/148991766", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-148991766", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 148991766, "node_id": "MDEyOklzc3VlQ29tbWVudDE0ODk5MTc2Ng==", "user": {"login": "honnibal", "id": 8059750, "node_id": "MDQ6VXNlcjgwNTk3NTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8059750?v=4", "gravatar_id": "", "url": "https://api.github.com/users/honnibal", "html_url": "https://github.com/honnibal", "followers_url": "https://api.github.com/users/honnibal/followers", "following_url": "https://api.github.com/users/honnibal/following{/other_user}", "gists_url": "https://api.github.com/users/honnibal/gists{/gist_id}", "starred_url": "https://api.github.com/users/honnibal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/honnibal/subscriptions", "organizations_url": "https://api.github.com/users/honnibal/orgs", "repos_url": "https://api.github.com/users/honnibal/repos", "events_url": "https://api.github.com/users/honnibal/events{/privacy}", "received_events_url": "https://api.github.com/users/honnibal/received_events", "type": "User", "site_admin": false}, "created_at": "2015-10-18T08:46:48Z", "updated_at": "2015-10-18T08:46:48Z", "author_association": "MEMBER", "body": "Just pushed v0.95. Everything should now pickle.\n\nThe only known issue is that you shouldn't begin training a model and then pickle it part way through training. Please keep an eye out for other issues, and report them :)\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/150037232", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-150037232", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 150037232, "node_id": "MDEyOklzc3VlQ29tbWVudDE1MDAzNzIzMg==", "user": {"login": "chrisdubois", "id": 1958804, "node_id": "MDQ6VXNlcjE5NTg4MDQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/1958804?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chrisdubois", "html_url": "https://github.com/chrisdubois", "followers_url": "https://api.github.com/users/chrisdubois/followers", "following_url": "https://api.github.com/users/chrisdubois/following{/other_user}", "gists_url": "https://api.github.com/users/chrisdubois/gists{/gist_id}", "starred_url": "https://api.github.com/users/chrisdubois/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chrisdubois/subscriptions", "organizations_url": "https://api.github.com/users/chrisdubois/orgs", "repos_url": "https://api.github.com/users/chrisdubois/repos", "events_url": "https://api.github.com/users/chrisdubois/events{/privacy}", "received_events_url": "https://api.github.com/users/chrisdubois/received_events", "type": "User", "site_admin": false}, "created_at": "2015-10-21T22:00:10Z", "updated_at": "2015-10-21T22:00:10Z", "author_association": "CONTRIBUTOR", "body": "I'm trying this out with the following snippet\n\n```\nimport cloudpickle\nnlp = English()\np = cloudpickle.CloudPickler(open('tmp', 'w'))\np.dump(nlp)\np.close()\n```\n\nI get the following:\n\n```\nTypeError: can't pickle Tokenizer objects\n```\n\nIt seems like the pickler tries to pickle a Vocab, Tokenizer, Tagger, Parser, and Matcher object. When it gets to the [Tokenizer](https://github.com/honnibal/spaCy/blob/master/spacy/tokenizer.pyx#L22), it doesn't seem to have a doesn't seem to have a `__reduce__` implemented. I don't see Tokenizer in the above list of \"state objects that need pickling\"; do we not expect the pickler to find the Tokenizer object?\n\nThanks in advance for any hints.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/150157747", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-150157747", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 150157747, "node_id": "MDEyOklzc3VlQ29tbWVudDE1MDE1Nzc0Nw==", "user": {"login": "honnibal", "id": 8059750, "node_id": "MDQ6VXNlcjgwNTk3NTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8059750?v=4", "gravatar_id": "", "url": "https://api.github.com/users/honnibal", "html_url": "https://github.com/honnibal", "followers_url": "https://api.github.com/users/honnibal/followers", "following_url": "https://api.github.com/users/honnibal/following{/other_user}", "gists_url": "https://api.github.com/users/honnibal/gists{/gist_id}", "starred_url": "https://api.github.com/users/honnibal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/honnibal/subscriptions", "organizations_url": "https://api.github.com/users/honnibal/orgs", "repos_url": "https://api.github.com/users/honnibal/repos", "events_url": "https://api.github.com/users/honnibal/events{/privacy}", "received_events_url": "https://api.github.com/users/honnibal/received_events", "type": "User", "site_admin": false}, "created_at": "2015-10-22T09:14:33Z", "updated_at": "2015-10-22T09:14:33Z", "author_association": "MEMBER", "body": "This is odd \u2014 I definitely thought I'd done this. And I don't get how my tests passed if the Tokenizer doesn't pickle. I'll figure this out, thanks.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/150309021", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-150309021", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 150309021, "node_id": "MDEyOklzc3VlQ29tbWVudDE1MDMwOTAyMQ==", "user": {"login": "chrisdubois", "id": 1958804, "node_id": "MDQ6VXNlcjE5NTg4MDQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/1958804?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chrisdubois", "html_url": "https://github.com/chrisdubois", "followers_url": "https://api.github.com/users/chrisdubois/followers", "following_url": "https://api.github.com/users/chrisdubois/following{/other_user}", "gists_url": "https://api.github.com/users/chrisdubois/gists{/gist_id}", "starred_url": "https://api.github.com/users/chrisdubois/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chrisdubois/subscriptions", "organizations_url": "https://api.github.com/users/chrisdubois/orgs", "repos_url": "https://api.github.com/users/chrisdubois/repos", "events_url": "https://api.github.com/users/chrisdubois/events{/privacy}", "received_events_url": "https://api.github.com/users/chrisdubois/received_events", "type": "User", "site_admin": false}, "created_at": "2015-10-22T18:09:17Z", "updated_at": "2015-10-22T18:09:17Z", "author_association": "CONTRIBUTOR", "body": "FWIW: tests/test_pickle.py passes for me as well. There seems to be something different between `p.dump(object)` and `p.dump(object, file)` that I don't understand. \n\nAlso, my test code above wasn't quite correct nor complete. It should be:\n\n```\nimport spacy\nfrom spacy.en import English\nnlp = English()\nimport cloudpickle\nf = open('tmp', 'wb')\np = cloudpickle.CloudPickler(f)\np.dump(nlp)\nf.close()\n```\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/150761379", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-150761379", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 150761379, "node_id": "MDEyOklzc3VlQ29tbWVudDE1MDc2MTM3OQ==", "user": {"login": "chrisdubois", "id": 1958804, "node_id": "MDQ6VXNlcjE5NTg4MDQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/1958804?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chrisdubois", "html_url": "https://github.com/chrisdubois", "followers_url": "https://api.github.com/users/chrisdubois/followers", "following_url": "https://api.github.com/users/chrisdubois/following{/other_user}", "gists_url": "https://api.github.com/users/chrisdubois/gists{/gist_id}", "starred_url": "https://api.github.com/users/chrisdubois/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chrisdubois/subscriptions", "organizations_url": "https://api.github.com/users/chrisdubois/orgs", "repos_url": "https://api.github.com/users/chrisdubois/repos", "events_url": "https://api.github.com/users/chrisdubois/events{/privacy}", "received_events_url": "https://api.github.com/users/chrisdubois/received_events", "type": "User", "site_admin": false}, "created_at": "2015-10-24T06:09:45Z", "updated_at": "2015-10-24T06:10:26Z", "author_association": "CONTRIBUTOR", "body": "I made a PR #149 that appears to pickle the tokenizer a bit better, but if I try and query for vectors from the deserialized model I get\n\n```\n>   raise ValueError(\nE   ValueError: Word vectors set to length 0. This may be because the data is not installed. If you haven't already, run\nE   python -m spacy.en.download all\nE   to install the data.\n\nspacy/tokens/token.pyx:149: ValueError\n========================================================================================================= 1 failed in 30.63 seconds ==========================================================================================================\n```\n\neven though I'm certain I've downloaded data.\n\nThen I found [spacy/vocab.pyx:108](https://github.com/honnibal/spaCy/blob/master/spacy/vocab.pyx#L108) which says `# TODO: Dump vectors`. Any suggestions on what needs to be done to save the vectors?\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/150998809", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-150998809", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 150998809, "node_id": "MDEyOklzc3VlQ29tbWVudDE1MDk5ODgwOQ==", "user": {"login": "honnibal", "id": 8059750, "node_id": "MDQ6VXNlcjgwNTk3NTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8059750?v=4", "gravatar_id": "", "url": "https://api.github.com/users/honnibal", "html_url": "https://github.com/honnibal", "followers_url": "https://api.github.com/users/honnibal/followers", "following_url": "https://api.github.com/users/honnibal/following{/other_user}", "gists_url": "https://api.github.com/users/honnibal/gists{/gist_id}", "starred_url": "https://api.github.com/users/honnibal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/honnibal/subscriptions", "organizations_url": "https://api.github.com/users/honnibal/orgs", "repos_url": "https://api.github.com/users/honnibal/repos", "events_url": "https://api.github.com/users/honnibal/events{/privacy}", "received_events_url": "https://api.github.com/users/honnibal/received_events", "type": "User", "site_admin": false}, "created_at": "2015-10-26T01:33:54Z", "updated_at": "2015-10-26T01:33:54Z", "author_association": "MEMBER", "body": "Fixed \u2014 thanks.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/172863116", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-172863116", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 172863116, "node_id": "MDEyOklzc3VlQ29tbWVudDE3Mjg2MzExNg==", "user": {"login": "honnibal", "id": 8059750, "node_id": "MDQ6VXNlcjgwNTk3NTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8059750?v=4", "gravatar_id": "", "url": "https://api.github.com/users/honnibal", "html_url": "https://github.com/honnibal", "followers_url": "https://api.github.com/users/honnibal/followers", "following_url": "https://api.github.com/users/honnibal/following{/other_user}", "gists_url": "https://api.github.com/users/honnibal/gists{/gist_id}", "starred_url": "https://api.github.com/users/honnibal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/honnibal/subscriptions", "organizations_url": "https://api.github.com/users/honnibal/orgs", "repos_url": "https://api.github.com/users/honnibal/repos", "events_url": "https://api.github.com/users/honnibal/events{/privacy}", "received_events_url": "https://api.github.com/users/honnibal/received_events", "type": "User", "site_admin": false}, "created_at": "2016-01-19T14:05:22Z", "updated_at": "2016-01-19T14:05:22Z", "author_association": "MEMBER", "body": "Reopening this. The current pickling implementation was only supposed to be an exploratory kludge. However, I didn't leave a TODO and the status of it got lost.\n\n`Vocab.__reduce__` currently writes state to temp files, which are then not cleaned up. Pickling therefore fills the disk, and really only pretends to work.\n\nThe root of the problem is that a number of spaCy classes carry large binary data structures. Common usage is to load this data and consider it immutable, however you can write to these models, e.g. to change the word vectors, and pickle should not silently dump these changes. On the other hand it's harsh to assume we always need to write out the state. This would mean that users who follow the pattern of keeping the data immutable have to write out ~1gb of data to pickle the models. This makes average usage of Spark etc really problematic.\n\nWe could do this implicitly with copy-on-write semantics, but I don't think it's great to invoke some method where it may or may not write out 1gb of data to disk, depending on the entire execution history of the program.\n\nWe could have a more explicit version of copy-on-write, where all the classes track whether they've been changed, and then the models should refuse to be pickled if the state is unclean. Users would then explicitly save the state after they change it. I think this is a recipe for having long-running processes suddenly die, though. Mostly Python is designed around the assumption that things can either be pickled or they can't. It's surprising to find that your pickle works, sometimes, depending on state, but then your long-running process dies because you didn't meet the assumed invariant. And then next time you run, you get an error in that other place in your code where the classes get pickled.\n\nI've been thinking for a while that context managers are the idiomatic standard for dealing with this problem. The idea would be that if you want to write to any of this loaded data, you have to open it within a context manager, so that the changes are explicitly scoped, and you explicitly decide whether you want to save the changes or dump them.\n\nIgnoring the naming of everything, this might look like:\n\n``` python\nfrom spacy.en import English\n\nnlp = English()\n\n# Open a pre-trained model, do some more training, and save the changes\nwith nlp.entity.update_model(file_or_path_or_etc):\n    for doc, labels in my_training_data:\n        nlp.entity.train(doc, labels)\n\n# Change the vector of 'submarines' to be the vector formed\n# by \"spaceships - space + ocean\"\n# When the context manager exits, revert the changes\nwith nlp.vocab.update_lexicon(revert=True):\n    submarines = nlp.vocab[u'submarines']\n    spaceships = nlp.vocab[u'spaceships']\n    space = nlp.vocab[u'space']\n    ocean = nlp.vocab[u'ocean']\n    submarines.vector = (spaceships.vector - space.vector) + ocean.vector\n```\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/172970071", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-172970071", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 172970071, "node_id": "MDEyOklzc3VlQ29tbWVudDE3Mjk3MDA3MQ==", "user": {"login": "aborsu", "id": 5033617, "node_id": "MDQ6VXNlcjUwMzM2MTc=", "avatar_url": "https://avatars0.githubusercontent.com/u/5033617?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aborsu", "html_url": "https://github.com/aborsu", "followers_url": "https://api.github.com/users/aborsu/followers", "following_url": "https://api.github.com/users/aborsu/following{/other_user}", "gists_url": "https://api.github.com/users/aborsu/gists{/gist_id}", "starred_url": "https://api.github.com/users/aborsu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aborsu/subscriptions", "organizations_url": "https://api.github.com/users/aborsu/orgs", "repos_url": "https://api.github.com/users/aborsu/repos", "events_url": "https://api.github.com/users/aborsu/events{/privacy}", "received_events_url": "https://api.github.com/users/aborsu/received_events", "type": "User", "site_admin": false}, "created_at": "2016-01-19T20:04:16Z", "updated_at": "2016-01-19T20:04:16Z", "author_association": "NONE", "body": "Having a bit of experience in spark, using Spacy with spark will always be tricky as the stringstore will evolve differently in all the workers so a string converted to an int will not necessarily be equivalent to the same int on another worker. So any shuffling of Spacy data across nodes will result in strange things.\n\nKnowing this, serializing the English instance is of little interest. To avoid dependency hell (making sure the same version of spacy and it's data is installed on each node), a much better approach would be to broadcast the library and the data models as broadcast objec (which takes less space and is sent as a torrent file)t, un-serialize them in a temp directory and lazy load them from there.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/172972136", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-172972136", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 172972136, "node_id": "MDEyOklzc3VlQ29tbWVudDE3Mjk3MjEzNg==", "user": {"login": "honnibal", "id": 8059750, "node_id": "MDQ6VXNlcjgwNTk3NTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8059750?v=4", "gravatar_id": "", "url": "https://api.github.com/users/honnibal", "html_url": "https://github.com/honnibal", "followers_url": "https://api.github.com/users/honnibal/followers", "following_url": "https://api.github.com/users/honnibal/following{/other_user}", "gists_url": "https://api.github.com/users/honnibal/gists{/gist_id}", "starred_url": "https://api.github.com/users/honnibal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/honnibal/subscriptions", "organizations_url": "https://api.github.com/users/honnibal/orgs", "repos_url": "https://api.github.com/users/honnibal/repos", "events_url": "https://api.github.com/users/honnibal/events{/privacy}", "received_events_url": "https://api.github.com/users/honnibal/received_events", "type": "User", "site_admin": false}, "created_at": "2016-01-19T20:12:04Z", "updated_at": "2016-01-19T20:18:18Z", "author_association": "MEMBER", "body": "That's a problem for some usages, yes. But the tokenizer is over 100x faster than the rest of the library, so you can simply tokenize all the text in a single process, to initialize the `StringStore` before you send out the batches.\n\nThere will also be use-cases where you only care that the activity on each shard is internally consistent. For instance, if you want to just recognise entities and store them in textual form, you don't care that the `StringStore` instances can diverge between your shards.\n\nWe think it's important to support Spark as best we can. We can at least pickle our data correctly :)\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/172974817", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-172974817", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 172974817, "node_id": "MDEyOklzc3VlQ29tbWVudDE3Mjk3NDgxNw==", "user": {"login": "aborsu", "id": 5033617, "node_id": "MDQ6VXNlcjUwMzM2MTc=", "avatar_url": "https://avatars0.githubusercontent.com/u/5033617?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aborsu", "html_url": "https://github.com/aborsu", "followers_url": "https://api.github.com/users/aborsu/followers", "following_url": "https://api.github.com/users/aborsu/following{/other_user}", "gists_url": "https://api.github.com/users/aborsu/gists{/gist_id}", "starred_url": "https://api.github.com/users/aborsu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aborsu/subscriptions", "organizations_url": "https://api.github.com/users/aborsu/orgs", "repos_url": "https://api.github.com/users/aborsu/repos", "events_url": "https://api.github.com/users/aborsu/events{/privacy}", "received_events_url": "https://api.github.com/users/aborsu/received_events", "type": "User", "site_admin": false}, "created_at": "2016-01-19T20:22:44Z", "updated_at": "2016-01-19T20:58:38Z", "author_association": "NONE", "body": "I agree with both things you said, you could indeed tokenize all texts in one process before sending them out, but if you need to use Spark (and I mean need not want to because it's cool), chances are that this would still be blocking as all the data would have to go through a single node.\nAnd for your second use case, I still think that bundling the Spacy data and classes then lazy loading them would be easier than adding serialization to all your classes.\n(I'm saying that because before playing with Spacy on apache-storm, I painfully implemented a serializable nlp pipeline in scala on Spark.)\nExtract from the [spark 1.6.0 manual](http://spark.apache.org/docs/latest/submitting-applications.html)\n\n```\nFor Python, you can use the --py-files argument of spark-submit to \nadd .py, .zip or .egg files to be distributed with your application. \nIf you depend on multiple Python files we recommend packaging them\ninto a .zip or .egg.\n```\n\n**edit** Also, Spacy uses compiled C-libraries that need to be installed on all the nodes. So installing Spacy on all the machines and just packaging the data to be lazy-loaded seems the easiest solution to me. [confer](http://blog.cloudera.com/blog/2015/09/how-to-prepare-your-apache-hadoop-cluster-for-pyspark-jobs/)\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/172977679", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-172977679", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 172977679, "node_id": "MDEyOklzc3VlQ29tbWVudDE3Mjk3NzY3OQ==", "user": {"login": "honnibal", "id": 8059750, "node_id": "MDQ6VXNlcjgwNTk3NTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8059750?v=4", "gravatar_id": "", "url": "https://api.github.com/users/honnibal", "html_url": "https://github.com/honnibal", "followers_url": "https://api.github.com/users/honnibal/followers", "following_url": "https://api.github.com/users/honnibal/following{/other_user}", "gists_url": "https://api.github.com/users/honnibal/gists{/gist_id}", "starred_url": "https://api.github.com/users/honnibal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/honnibal/subscriptions", "organizations_url": "https://api.github.com/users/honnibal/orgs", "repos_url": "https://api.github.com/users/honnibal/repos", "events_url": "https://api.github.com/users/honnibal/events{/privacy}", "received_events_url": "https://api.github.com/users/honnibal/received_events", "type": "User", "site_admin": false}, "created_at": "2016-01-19T20:34:05Z", "updated_at": "2016-01-19T20:34:05Z", "author_association": "MEMBER", "body": "Hmm. I do see your argument.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/182445765", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-182445765", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 182445765, "node_id": "MDEyOklzc3VlQ29tbWVudDE4MjQ0NTc2NQ==", "user": {"login": "honnibal", "id": 8059750, "node_id": "MDQ6VXNlcjgwNTk3NTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8059750?v=4", "gravatar_id": "", "url": "https://api.github.com/users/honnibal", "html_url": "https://github.com/honnibal", "followers_url": "https://api.github.com/users/honnibal/followers", "following_url": "https://api.github.com/users/honnibal/following{/other_user}", "gists_url": "https://api.github.com/users/honnibal/gists{/gist_id}", "starred_url": "https://api.github.com/users/honnibal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/honnibal/subscriptions", "organizations_url": "https://api.github.com/users/honnibal/orgs", "repos_url": "https://api.github.com/users/honnibal/repos", "events_url": "https://api.github.com/users/honnibal/events{/privacy}", "received_events_url": "https://api.github.com/users/honnibal/received_events", "type": "User", "site_admin": false}, "created_at": "2016-02-10T16:00:40Z", "updated_at": "2016-02-10T16:00:40Z", "author_association": "MEMBER", "body": "With the new improvements in loading time (down to 15s from about 100s), and the addition of multi-threading, I'm not nearly as keen on supporting in-memory pickling as I was before.  I think @aborsu is right: the problem of serializing such that pickle can work is basically the same as the problem of loading from disk, so there's no reason to believe unpickling the classes would be faster than just loading them as new objects.\n\nThe important thing is to have a workflow for users that want to use spaCy with a map/reduce framework like Spark. I think supporting the pickle protocol is not necessarily the best way to do that. Thoughts?\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/185881231", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-185881231", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 185881231, "node_id": "MDEyOklzc3VlQ29tbWVudDE4NTg4MTIzMQ==", "user": {"login": "mikepb", "id": 637039, "node_id": "MDQ6VXNlcjYzNzAzOQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/637039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mikepb", "html_url": "https://github.com/mikepb", "followers_url": "https://api.github.com/users/mikepb/followers", "following_url": "https://api.github.com/users/mikepb/following{/other_user}", "gists_url": "https://api.github.com/users/mikepb/gists{/gist_id}", "starred_url": "https://api.github.com/users/mikepb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mikepb/subscriptions", "organizations_url": "https://api.github.com/users/mikepb/orgs", "repos_url": "https://api.github.com/users/mikepb/repos", "events_url": "https://api.github.com/users/mikepb/events{/privacy}", "received_events_url": "https://api.github.com/users/mikepb/received_events", "type": "User", "site_admin": false}, "created_at": "2016-02-18T19:32:39Z", "updated_at": "2016-02-18T19:32:39Z", "author_association": "NONE", "body": "I also had issues with pickling with [SFrame](https://github.com/dato-code/SFrame), which behaves similarly to how Spark has been described to work here.\n\nTo avoid pickling and unpickling data that could just be read from disk (on a single machine), I've been using the following `Pickleless` proxy object.\n\n``` py\nclass Pickleless(object):\n\n    def __init__(self, cls, *args, **kwargs):\n        object.__setattr__(self, '__init', (cls, args, kwargs))\n\n    def __o(self):\n        try:\n            o = self.__dict__['__instance']\n        except KeyError:\n            cls, args, kwargs = self.__dict__['__init']\n            o = cls(*args, **kwargs)\n            object.__setattr__(self, '__instance', o)\n        return o\n\n    def __getstate__(self):\n        return self.__dict__['__init']\n\n    def __setstate__(self, state):\n        cls, args, kwargs = state\n        Pickleless.__init__(self, cls, *args, **kwargs)\n\n    def __call__(self, *args, **kwargs):\n        return self.__o()(*args, **kwargs)\n\n    def __getattr__(self, name):\n        return getattr(self.__o(), name)\n\n    def __setattr__(self, name, value):\n        setattr(self.__o(), name, value)\n\n_English = Pickleless(spacy.en.English)\ndef nlp(text, **kwargs):\n    return _English(_decode(text), **kwargs)\n```\n\nI had originally implemented the proxy using thread-local storage, but this works just as well for my purposes.\n\nJust my 2\u00a2...\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/195053899", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-195053899", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 195053899, "node_id": "MDEyOklzc3VlQ29tbWVudDE5NTA1Mzg5OQ==", "user": {"login": "gushecht", "id": 2095663, "node_id": "MDQ6VXNlcjIwOTU2NjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/2095663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gushecht", "html_url": "https://github.com/gushecht", "followers_url": "https://api.github.com/users/gushecht/followers", "following_url": "https://api.github.com/users/gushecht/following{/other_user}", "gists_url": "https://api.github.com/users/gushecht/gists{/gist_id}", "starred_url": "https://api.github.com/users/gushecht/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gushecht/subscriptions", "organizations_url": "https://api.github.com/users/gushecht/orgs", "repos_url": "https://api.github.com/users/gushecht/repos", "events_url": "https://api.github.com/users/gushecht/events{/privacy}", "received_events_url": "https://api.github.com/users/gushecht/received_events", "type": "User", "site_admin": false}, "created_at": "2016-03-10T21:18:32Z", "updated_at": "2016-03-10T21:18:32Z", "author_association": "CONTRIBUTOR", "body": "Hi guys,\n\nRelative noob here looking for a recommendation.\n\nMy goal: POS tag the English language Wikipedia dump (approx 23GB, maybe 3B words).\n\nCan someone tell me either:\nA) DON'T DO IT, go for something smaller; or\nB) A distillation of the above discussion into the current recommended workflow?  Unfortunately, I can't quite follow the conversation.\n\nP.S. Thanks Matthew for spaCy!  I'm using that and following your sense2vec implementation, with a few changes, to attempt the [word clustering that is mentioned on Google Code word2vec page](https://code.google.com/archive/p/word2vec/).\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/195058182", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-195058182", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 195058182, "node_id": "MDEyOklzc3VlQ29tbWVudDE5NTA1ODE4Mg==", "user": {"login": "mikepb", "id": 637039, "node_id": "MDQ6VXNlcjYzNzAzOQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/637039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mikepb", "html_url": "https://github.com/mikepb", "followers_url": "https://api.github.com/users/mikepb/followers", "following_url": "https://api.github.com/users/mikepb/following{/other_user}", "gists_url": "https://api.github.com/users/mikepb/gists{/gist_id}", "starred_url": "https://api.github.com/users/mikepb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mikepb/subscriptions", "organizations_url": "https://api.github.com/users/mikepb/orgs", "repos_url": "https://api.github.com/users/mikepb/repos", "events_url": "https://api.github.com/users/mikepb/events{/privacy}", "received_events_url": "https://api.github.com/users/mikepb/received_events", "type": "User", "site_admin": false}, "created_at": "2016-03-10T21:31:00Z", "updated_at": "2016-03-10T21:32:43Z", "author_association": "NONE", "body": "I was planning on doing that until... I ran out of memory. I was using the scikit-learn word count module, so YMMV. They also have a good implementation of word2vec, but it unfortunately the scikit pipeline does not always use iterators and would need quite a bit of memory (>24GB including paging when I tried a _much_ smaller dataset).\n\nI recommend starting with something smaller, such as using only the Wikipedia page titles, to test out your pipeline. Debugging would go more quickly too :+1: \n\nIf you are on a Mac, you can enable multithreading per the instructions in #267 .\n\nThis conversation is about distributing spacy over multiple machines. The problem is not that it doesn't work, but that the distributed computing frameworks mentioned in this thread pickle and send over the spacy data models. That could amount to several GB of data. Instead, workarounds have been posted to ensure that spacy loads its models from local disk on each machine.\n\nThe workaround also means that you won't be able to share spacy states among machine, including custom dictionaries.\n\nHope this helps!\n\nCheers.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/195058679", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-195058679", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 195058679, "node_id": "MDEyOklzc3VlQ29tbWVudDE5NTA1ODY3OQ==", "user": {"login": "honnibal", "id": 8059750, "node_id": "MDQ6VXNlcjgwNTk3NTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8059750?v=4", "gravatar_id": "", "url": "https://api.github.com/users/honnibal", "html_url": "https://github.com/honnibal", "followers_url": "https://api.github.com/users/honnibal/followers", "following_url": "https://api.github.com/users/honnibal/following{/other_user}", "gists_url": "https://api.github.com/users/honnibal/gists{/gist_id}", "starred_url": "https://api.github.com/users/honnibal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/honnibal/subscriptions", "organizations_url": "https://api.github.com/users/honnibal/orgs", "repos_url": "https://api.github.com/users/honnibal/repos", "events_url": "https://api.github.com/users/honnibal/events{/privacy}", "received_events_url": "https://api.github.com/users/honnibal/received_events", "type": "User", "site_admin": false}, "created_at": "2016-03-10T21:32:53Z", "updated_at": "2016-03-10T21:32:53Z", "author_association": "MEMBER", "body": "Tagging Wikipedia should be no problem. I haven't benchmarked the tagger in a while, but I think it should be running at over 100k tokens a second per process, and it doesn't use much memory. On a high compute node you should be done in a couple of hours.\n\nI've run the full pipeline on over ten times that much data, and I don't use Spark --- just simple multiprocessing, on a node with lots of cores.\n\nExample script: https://github.com/spacy-io/spaCy/blob/master/examples/parallel_parse.py\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/195059296", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-195059296", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 195059296, "node_id": "MDEyOklzc3VlQ29tbWVudDE5NTA1OTI5Ng==", "user": {"login": "gushecht", "id": 2095663, "node_id": "MDQ6VXNlcjIwOTU2NjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/2095663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gushecht", "html_url": "https://github.com/gushecht", "followers_url": "https://api.github.com/users/gushecht/followers", "following_url": "https://api.github.com/users/gushecht/following{/other_user}", "gists_url": "https://api.github.com/users/gushecht/gists{/gist_id}", "starred_url": "https://api.github.com/users/gushecht/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gushecht/subscriptions", "organizations_url": "https://api.github.com/users/gushecht/orgs", "repos_url": "https://api.github.com/users/gushecht/repos", "events_url": "https://api.github.com/users/gushecht/events{/privacy}", "received_events_url": "https://api.github.com/users/gushecht/received_events", "type": "User", "site_admin": false}, "created_at": "2016-03-10T21:35:08Z", "updated_at": "2016-03-10T21:35:08Z", "author_association": "CONTRIBUTOR", "body": "Thanks both of you.  I was thinking Spark because of the benchmarking on this [blog post](https://spacy.io/blog/part-of-speech-POS-tagger-in-python) suggested I'd be at it for about half a year (although I also may have misinterpreted the results).\n\nWill continue onwards and let you know about my results when I get them!\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/195059392", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-195059392", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 195059392, "node_id": "MDEyOklzc3VlQ29tbWVudDE5NTA1OTM5Mg==", "user": {"login": "mikepb", "id": 637039, "node_id": "MDQ6VXNlcjYzNzAzOQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/637039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mikepb", "html_url": "https://github.com/mikepb", "followers_url": "https://api.github.com/users/mikepb/followers", "following_url": "https://api.github.com/users/mikepb/following{/other_user}", "gists_url": "https://api.github.com/users/mikepb/gists{/gist_id}", "starred_url": "https://api.github.com/users/mikepb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mikepb/subscriptions", "organizations_url": "https://api.github.com/users/mikepb/orgs", "repos_url": "https://api.github.com/users/mikepb/repos", "events_url": "https://api.github.com/users/mikepb/events{/privacy}", "received_events_url": "https://api.github.com/users/mikepb/received_events", "type": "User", "site_admin": false}, "created_at": "2016-03-10T21:35:31Z", "updated_at": "2016-03-10T21:35:31Z", "author_association": "NONE", "body": "Second what @honnibal said. The issues I ran into were outside of spacy in doing the word count. You could try the [HashingVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer) in sklearn, though I had issues with that too.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/195124289", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-195124289", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 195124289, "node_id": "MDEyOklzc3VlQ29tbWVudDE5NTEyNDI4OQ==", "user": {"login": "gushecht", "id": 2095663, "node_id": "MDQ6VXNlcjIwOTU2NjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/2095663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gushecht", "html_url": "https://github.com/gushecht", "followers_url": "https://api.github.com/users/gushecht/followers", "following_url": "https://api.github.com/users/gushecht/following{/other_user}", "gists_url": "https://api.github.com/users/gushecht/gists{/gist_id}", "starred_url": "https://api.github.com/users/gushecht/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gushecht/subscriptions", "organizations_url": "https://api.github.com/users/gushecht/orgs", "repos_url": "https://api.github.com/users/gushecht/repos", "events_url": "https://api.github.com/users/gushecht/events{/privacy}", "received_events_url": "https://api.github.com/users/gushecht/received_events", "type": "User", "site_admin": false}, "created_at": "2016-03-11T01:06:14Z", "updated_at": "2016-03-11T01:06:14Z", "author_association": "CONTRIBUTOR", "body": "Hey @honnibal, before going onto AWS, I wanted to start small.  I took your merge_text.py from the sense2vec implementation and trimmed it down.  Running it on a pretty small input is taking a while.  Can you see something that I'm doing wrong?  Updated script and data file attached.\n\nI'm on a 4 core 2011 MacBook Pro with 8GB RAM, so it shouldn't be the hardware.\n[Archive.zip](https://github.com/spacy-io/spaCy/files/168294/Archive.zip)\n\nThanks!\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/195136000", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-195136000", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 195136000, "node_id": "MDEyOklzc3VlQ29tbWVudDE5NTEzNjAwMA==", "user": {"login": "mikepb", "id": 637039, "node_id": "MDQ6VXNlcjYzNzAzOQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/637039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mikepb", "html_url": "https://github.com/mikepb", "followers_url": "https://api.github.com/users/mikepb/followers", "following_url": "https://api.github.com/users/mikepb/following{/other_user}", "gists_url": "https://api.github.com/users/mikepb/gists{/gist_id}", "starred_url": "https://api.github.com/users/mikepb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mikepb/subscriptions", "organizations_url": "https://api.github.com/users/mikepb/orgs", "repos_url": "https://api.github.com/users/mikepb/repos", "events_url": "https://api.github.com/users/mikepb/events{/privacy}", "received_events_url": "https://api.github.com/users/mikepb/received_events", "type": "User", "site_admin": false}, "created_at": "2016-03-11T01:36:16Z", "updated_at": "2016-03-11T01:36:16Z", "author_association": "NONE", "body": "Seems to me that Parallel is doing the pickle thing mentioned in this thread.\n\nThis means that each worker process will take a while to load all the data models. On my MacBook Pro, each Python worker process uses between 800MB and 1.5GB. I have 16GB physical memory and an SSD, so I don't notice paging as much. Apple didn't make SSD MBP in 2011, so it's probably going to be much slower for you. I stopped your script after spending a few minutes writing this post.\n\nThe overhead of pickling and unpickling the spacy data models is significant. As spacy itself is very fast, it would take huge batch sizes to make it worth it to use this multiprocessing model.\n\nInstead, it would be significantly faster to use a single Python process and spacy's `nlp.pipe(docs, n_threads=-1)` multithreading API. On Linux, this is supported natively. On Mac, you'll need to do a little more work (#267). I modified your script to not use Parallel and it ran to completion in seconds, single-threaded.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/195645823", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-195645823", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 195645823, "node_id": "MDEyOklzc3VlQ29tbWVudDE5NTY0NTgyMw==", "user": {"login": "rw", "id": 21367, "node_id": "MDQ6VXNlcjIxMzY3", "avatar_url": "https://avatars2.githubusercontent.com/u/21367?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rw", "html_url": "https://github.com/rw", "followers_url": "https://api.github.com/users/rw/followers", "following_url": "https://api.github.com/users/rw/following{/other_user}", "gists_url": "https://api.github.com/users/rw/gists{/gist_id}", "starred_url": "https://api.github.com/users/rw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rw/subscriptions", "organizations_url": "https://api.github.com/users/rw/orgs", "repos_url": "https://api.github.com/users/rw/repos", "events_url": "https://api.github.com/users/rw/events{/privacy}", "received_events_url": "https://api.github.com/users/rw/received_events", "type": "User", "site_admin": false}, "created_at": "2016-03-12T02:56:55Z", "updated_at": "2016-03-12T02:56:55Z", "author_association": "CONTRIBUTOR", "body": "Would a different serialization format help here? Pickle is heavyweight.\nThere are many to choose from.\n\nI maintain the Python port of FlatBuffers, which is a serialization format\noptimized for minimal memory usage. Happy to answer any questions.\nOn Mar 10, 2016 5:36 PM, \"mikepb\" notifications@github.com wrote:\n\n> Seems to me that Parallel is doing the pickle thing mentioned in this\n> thread.\n> \n> This means that each worker process will take a while to load all the data\n> models. On my MacBook Pro, each Python worker process uses between 800MB\n> and 1.5GB. I have 16GB physical memory and an SSD, so I don't notice paging\n> as much. Apple didn't make SSD MBP in 2011, so it's probably going to be\n> much slower for you. I stopped your script after spending a few minutes\n> writing this post.\n> \n> The overhead of pickling and unpickling the spacy data models is\n> significant. As spacy itself is very fast, it would take huge batch sizes\n> to make it worth it to use this multiprocessing model.\n> \n> Instead, it would be significantly faster to use a single Python process\n> and spacy's nlp.pipe(docs, n_threads=-1) multithreading API. On Linux,\n> this is supported natively. On Mac, you'll need to do a little more work (\n> #267 https://github.com/spacy-io/spaCy/issues/267). I modified your\n> script to not use Parallel and it ran to completion in seconds,\n> single-threaded.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/spacy-io/spaCy/issues/125#issuecomment-195136000.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/207134273", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-207134273", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 207134273, "node_id": "MDEyOklzc3VlQ29tbWVudDIwNzEzNDI3Mw==", "user": {"login": "sjjpo2002", "id": 8649062, "node_id": "MDQ6VXNlcjg2NDkwNjI=", "avatar_url": "https://avatars3.githubusercontent.com/u/8649062?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sjjpo2002", "html_url": "https://github.com/sjjpo2002", "followers_url": "https://api.github.com/users/sjjpo2002/followers", "following_url": "https://api.github.com/users/sjjpo2002/following{/other_user}", "gists_url": "https://api.github.com/users/sjjpo2002/gists{/gist_id}", "starred_url": "https://api.github.com/users/sjjpo2002/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sjjpo2002/subscriptions", "organizations_url": "https://api.github.com/users/sjjpo2002/orgs", "repos_url": "https://api.github.com/users/sjjpo2002/repos", "events_url": "https://api.github.com/users/sjjpo2002/events{/privacy}", "received_events_url": "https://api.github.com/users/sjjpo2002/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-07T23:16:47Z", "updated_at": "2016-04-07T23:16:47Z", "author_association": "CONTRIBUTOR", "body": "I ran into this issue too. I'm trying to do some work on the text from Wiki Dump. Using a Spark map function do_work. As suggested here when I move English() inside the function it works but when it's outside it doesn't. Python crashes. But instantiating English() in every function call makes it super slow and practically useless. \n\n```\n `def do_work(batch_of_texts):\n    nlp = English()\n    for text in texts:\n           doc = nlp(text)`\n```\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/207134890", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-207134890", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 207134890, "node_id": "MDEyOklzc3VlQ29tbWVudDIwNzEzNDg5MA==", "user": {"login": "mikepb", "id": 637039, "node_id": "MDQ6VXNlcjYzNzAzOQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/637039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mikepb", "html_url": "https://github.com/mikepb", "followers_url": "https://api.github.com/users/mikepb/followers", "following_url": "https://api.github.com/users/mikepb/following{/other_user}", "gists_url": "https://api.github.com/users/mikepb/gists{/gist_id}", "starred_url": "https://api.github.com/users/mikepb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mikepb/subscriptions", "organizations_url": "https://api.github.com/users/mikepb/orgs", "repos_url": "https://api.github.com/users/mikepb/repos", "events_url": "https://api.github.com/users/mikepb/events{/privacy}", "received_events_url": "https://api.github.com/users/mikepb/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-07T23:20:52Z", "updated_at": "2016-04-07T23:20:52Z", "author_association": "NONE", "body": "I've had success using the `Pickeless` workaround in https://github.com/spacy-io/spaCy/issues/125#issuecomment-185881231\n\nThe wrapper class essentially tells Python not to pickle the English. Instead, the Pickleless English are reloaded and cached for each process, when defined as a global variable.\n\nRemember that custom vocabulary will not transfer between Python processes using this method.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/207135641", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-207135641", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 207135641, "node_id": "MDEyOklzc3VlQ29tbWVudDIwNzEzNTY0MQ==", "user": {"login": "gushecht", "id": 2095663, "node_id": "MDQ6VXNlcjIwOTU2NjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/2095663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gushecht", "html_url": "https://github.com/gushecht", "followers_url": "https://api.github.com/users/gushecht/followers", "following_url": "https://api.github.com/users/gushecht/following{/other_user}", "gists_url": "https://api.github.com/users/gushecht/gists{/gist_id}", "starred_url": "https://api.github.com/users/gushecht/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gushecht/subscriptions", "organizations_url": "https://api.github.com/users/gushecht/orgs", "repos_url": "https://api.github.com/users/gushecht/repos", "events_url": "https://api.github.com/users/gushecht/events{/privacy}", "received_events_url": "https://api.github.com/users/gushecht/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-07T23:24:30Z", "updated_at": "2016-04-07T23:24:30Z", "author_association": "CONTRIBUTOR", "body": "I'm sure there's a better way to do this, but what I ended up doing was writing a script to process my corpus in parallel on one machine, instead of using Spark.  Each separate process had its own instantiation of the English object (so it was memory intensive) and a portion of the corpus.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/207148585", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-207148585", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 207148585, "node_id": "MDEyOklzc3VlQ29tbWVudDIwNzE0ODU4NQ==", "user": {"login": "sjjpo2002", "id": 8649062, "node_id": "MDQ6VXNlcjg2NDkwNjI=", "avatar_url": "https://avatars3.githubusercontent.com/u/8649062?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sjjpo2002", "html_url": "https://github.com/sjjpo2002", "followers_url": "https://api.github.com/users/sjjpo2002/followers", "following_url": "https://api.github.com/users/sjjpo2002/following{/other_user}", "gists_url": "https://api.github.com/users/sjjpo2002/gists{/gist_id}", "starred_url": "https://api.github.com/users/sjjpo2002/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sjjpo2002/subscriptions", "organizations_url": "https://api.github.com/users/sjjpo2002/orgs", "repos_url": "https://api.github.com/users/sjjpo2002/repos", "events_url": "https://api.github.com/users/sjjpo2002/events{/privacy}", "received_events_url": "https://api.github.com/users/sjjpo2002/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-08T00:31:01Z", "updated_at": "2016-04-08T00:31:01Z", "author_association": "CONTRIBUTOR", "body": "I don't know how but the Pickeless workaround does work. Python doesn't crash anymore but the process is still slow that I would guess it's normal for the large text that I'm dealing with\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/207166163", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-207166163", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 207166163, "node_id": "MDEyOklzc3VlQ29tbWVudDIwNzE2NjE2Mw==", "user": {"login": "honnibal", "id": 8059750, "node_id": "MDQ6VXNlcjgwNTk3NTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8059750?v=4", "gravatar_id": "", "url": "https://api.github.com/users/honnibal", "html_url": "https://github.com/honnibal", "followers_url": "https://api.github.com/users/honnibal/followers", "following_url": "https://api.github.com/users/honnibal/following{/other_user}", "gists_url": "https://api.github.com/users/honnibal/gists{/gist_id}", "starred_url": "https://api.github.com/users/honnibal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/honnibal/subscriptions", "organizations_url": "https://api.github.com/users/honnibal/orgs", "repos_url": "https://api.github.com/users/honnibal/repos", "events_url": "https://api.github.com/users/honnibal/events{/privacy}", "received_events_url": "https://api.github.com/users/honnibal/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-08T01:46:51Z", "updated_at": "2016-04-08T01:53:36Z", "author_association": "MEMBER", "body": "I really need to write a blog post/tutorial on working with large jobs. I think at the moment this is quite confusing. I'm grateful to @mikepb , @aborsu and others for their contributions to the discussion here.\n\n@sjjpo2002 : As a quick note, this doesn't change the deeper issues, but your code should be faster if you do:\n\n``` python\ndef do_work(batch_of_texts):\n    nlp = English()\n    for doc in nlp.pipe(batch_of_texts, n_threads=-1):\n        # Do stuff.\n```\n\nThis allows spaCy to make use of multi-threading for the most expensive parts of the pipeline, the parser and named entity recogniser. In future more of the pipeline will be multi-threaded, so I'd recommend using this construction even if you're just using the tokenizer and tagger, which are currently single-threaded (they're very fast though --- and load much quicker.)\n\nA much better improvement will be gained if you can manage to reuse the same `nlp` object across more text, i.e. if you can increase the size of your batches of documents.\n\nTo summarize the deeper issue as I understand it: the ultimate dream is to have `reduce(analyse_data, map(process_texts, texts))` \"Just Work\", without having to deal with the details of how work is allocated to physical machines, let alone individual processes or threads.\n\nUnfortunately, with spaCy (and probably with lots of other situations), the constants really matter. Loading the spaCy models takes about as much time as processing 1,000 documents (at ~200 tokens each). This means you have to somehow manually manage the chunk size. You have to somehow help Spark schedule the work in larger increments. If you have your data already split out into large files, e.g. by month, then it might be enough to have each process work on one month of data. In this situation, we can make `reduce(analyse_data, map(process_month, months))` work transparently, which is nice. But if what you have is a collection of texts, not a collection of larger archives, you have to think about the details a bit more.\n\nMy main advice would be to not assume that Spark, Hadoop etc are automatically better than using a single machine and using multiple processes with spaCy's `.pipe()` method for multithreading. In the worst case, if you had a chunk size of 1, you could use a thousand node cluster and have your job complete slower than it would on a Macbook Air. So, if your process is slow, it might be worth taking a closer look and checking that you're at least doing around 10,000 tokens per thread per second. If you're doing an order of magnitude less than that, then either the problem is outside of spaCy, or the work isn't being scheduled effectively.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/207235796", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-207235796", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 207235796, "node_id": "MDEyOklzc3VlQ29tbWVudDIwNzIzNTc5Ng==", "user": {"login": "sjjpo2002", "id": 8649062, "node_id": "MDQ6VXNlcjg2NDkwNjI=", "avatar_url": "https://avatars3.githubusercontent.com/u/8649062?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sjjpo2002", "html_url": "https://github.com/sjjpo2002", "followers_url": "https://api.github.com/users/sjjpo2002/followers", "following_url": "https://api.github.com/users/sjjpo2002/following{/other_user}", "gists_url": "https://api.github.com/users/sjjpo2002/gists{/gist_id}", "starred_url": "https://api.github.com/users/sjjpo2002/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sjjpo2002/subscriptions", "organizations_url": "https://api.github.com/users/sjjpo2002/orgs", "repos_url": "https://api.github.com/users/sjjpo2002/repos", "events_url": "https://api.github.com/users/sjjpo2002/events{/privacy}", "received_events_url": "https://api.github.com/users/sjjpo2002/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-08T06:20:33Z", "updated_at": "2016-04-08T06:20:33Z", "author_association": "CONTRIBUTOR", "body": "Thanks @honnibal for your advice. I've been trying to do NLP with Spark with NLTK and Stanford and etc. But their objects are big and not serializable. I really like Spacy's simple and all in one approach. It's close integration with Spark will be a great advantage for many researchers. As you mentioned that would be heaven to see this code just work:\n\n``` python\nfrom operator import add\nfrom pyspark import SparkContext, SparkConf\nfrom nltk.corpus import stopwords\nfrom spacy.en import English\n\nwiki_title_words = set(article_titles)\nenglish_vocab = set(w.lower() for w in nltk.corpus.words.words())\nstop_words = set(stopwords.words(\"english\"))\nnlp = English()\n\ndef lemmatize_filter(s):\n    candidates = nlp(s)\n    result = []\n    for candidate in candidates:\n        if (candidate.lemma_ not in stop_words) and \\\n           (candidate.pos_ != u'PUNCT') and \\\n           (candidate.lemma_ in english_vocab or \\\n            candidate.lemma_ in wiki_title_words):\n\n            result.append(candidate.lemma_)\n    return result\n\nsc = SparkContext()\nrawData = sc.textFile(r\"wiki\\artile_per_line.txt\") # title  \\t  article_body  \nflatten_words = rawData.flatMap(lambda x: lemmatize_filter(x.split('\\t')[1]))\nwords_reduced = flatten_words.map(lambda x: (x, 1)).reduceByKey(add)\nprint \"Number of distinct lemmas in the whole Wikipedia: \", words_reduced.count()\n```\n\nSo far it looks like using Spacy pipe without Spark is the fastest option to do the job. I will feed Spark with processed data for the rest of the work. \n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/207624154", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-207624154", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 207624154, "node_id": "MDEyOklzc3VlQ29tbWVudDIwNzYyNDE1NA==", "user": {"login": "sjjpo2002", "id": 8649062, "node_id": "MDQ6VXNlcjg2NDkwNjI=", "avatar_url": "https://avatars3.githubusercontent.com/u/8649062?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sjjpo2002", "html_url": "https://github.com/sjjpo2002", "followers_url": "https://api.github.com/users/sjjpo2002/followers", "following_url": "https://api.github.com/users/sjjpo2002/following{/other_user}", "gists_url": "https://api.github.com/users/sjjpo2002/gists{/gist_id}", "starred_url": "https://api.github.com/users/sjjpo2002/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sjjpo2002/subscriptions", "organizations_url": "https://api.github.com/users/sjjpo2002/orgs", "repos_url": "https://api.github.com/users/sjjpo2002/repos", "events_url": "https://api.github.com/users/sjjpo2002/events{/privacy}", "received_events_url": "https://api.github.com/users/sjjpo2002/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-08T22:01:33Z", "updated_at": "2016-04-08T22:01:33Z", "author_association": "CONTRIBUTOR", "body": "It might be off topic but @mikepb mentioned with `nlp.pipe([batch_docs], n_thread=-1` we can see a lot of improvement in speed. But I'm not gaining speed using pipe. I suspect that I have to enable multi-threading using OpenMP. But I couldn't find any instructions to do that on Windows. \n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/207625028", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-207625028", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 207625028, "node_id": "MDEyOklzc3VlQ29tbWVudDIwNzYyNTAyOA==", "user": {"login": "mikepb", "id": 637039, "node_id": "MDQ6VXNlcjYzNzAzOQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/637039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mikepb", "html_url": "https://github.com/mikepb", "followers_url": "https://api.github.com/users/mikepb/followers", "following_url": "https://api.github.com/users/mikepb/following{/other_user}", "gists_url": "https://api.github.com/users/mikepb/gists{/gist_id}", "starred_url": "https://api.github.com/users/mikepb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mikepb/subscriptions", "organizations_url": "https://api.github.com/users/mikepb/orgs", "repos_url": "https://api.github.com/users/mikepb/repos", "events_url": "https://api.github.com/users/mikepb/events{/privacy}", "received_events_url": "https://api.github.com/users/mikepb/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-08T22:05:51Z", "updated_at": "2016-04-08T22:05:51Z", "author_association": "NONE", "body": "Yes, you would need OpenMP to enable multithreading. It's tricky to get right:\n- See #267 and #266 for info on using OpenMP on OS X.\n\nIt's wonderful when it works though!\n\nI believe Microsoft has implemented OpenMP in Visual Studio. I'm wholly unfamiliar with Windows development...\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/207714899", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-207714899", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 207714899, "node_id": "MDEyOklzc3VlQ29tbWVudDIwNzcxNDg5OQ==", "user": {"login": "henningpeters", "id": 166570, "node_id": "MDQ6VXNlcjE2NjU3MA==", "avatar_url": "https://avatars3.githubusercontent.com/u/166570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/henningpeters", "html_url": "https://github.com/henningpeters", "followers_url": "https://api.github.com/users/henningpeters/followers", "following_url": "https://api.github.com/users/henningpeters/following{/other_user}", "gists_url": "https://api.github.com/users/henningpeters/gists{/gist_id}", "starred_url": "https://api.github.com/users/henningpeters/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/henningpeters/subscriptions", "organizations_url": "https://api.github.com/users/henningpeters/orgs", "repos_url": "https://api.github.com/users/henningpeters/repos", "events_url": "https://api.github.com/users/henningpeters/events{/privacy}", "received_events_url": "https://api.github.com/users/henningpeters/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-09T05:43:43Z", "updated_at": "2016-04-09T05:43:43Z", "author_association": "CONTRIBUTOR", "body": "@mikepb Normally, you don't have to do anything special on Windows, it's trickier on OS X because Apple's default clang doesn't support OpenMP, yet. MSVC does, though.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/207892428", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-207892428", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 207892428, "node_id": "MDEyOklzc3VlQ29tbWVudDIwNzg5MjQyOA==", "user": {"login": "sjjpo2002", "id": 8649062, "node_id": "MDQ6VXNlcjg2NDkwNjI=", "avatar_url": "https://avatars3.githubusercontent.com/u/8649062?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sjjpo2002", "html_url": "https://github.com/sjjpo2002", "followers_url": "https://api.github.com/users/sjjpo2002/followers", "following_url": "https://api.github.com/users/sjjpo2002/following{/other_user}", "gists_url": "https://api.github.com/users/sjjpo2002/gists{/gist_id}", "starred_url": "https://api.github.com/users/sjjpo2002/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sjjpo2002/subscriptions", "organizations_url": "https://api.github.com/users/sjjpo2002/orgs", "repos_url": "https://api.github.com/users/sjjpo2002/repos", "events_url": "https://api.github.com/users/sjjpo2002/events{/privacy}", "received_events_url": "https://api.github.com/users/sjjpo2002/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-10T00:54:20Z", "updated_at": "2016-04-10T01:11:33Z", "author_association": "CONTRIBUTOR", "body": "@henningpeters I managed to compile and run pipe with OpenMP support. It looks beautiful when it utilizes all CPU cores. OpenMP support is by default disabled for MSVC. I added the compiler option to the setup file and committed the changes to the master branch. I'm surprised nobody is using Windows it seems. But with that switch enabled next releases will work properly on Windows\n\n@honnibal What you mentioned about some parts of the pipeline multi-threaded made a lot more sense after I managed to use pipe in multi-threaded scenario. But it also seems single-threaded parts of the pipeline take almost the same time as the multi-threaded parts. That would make it a lot faster if those single threads also support multithreaded. I attach my CPU usage here. It's clear the single threads play some role in the time it takes to finish the job. \n\n![pipe_performance](https://cloud.githubusercontent.com/assets/8649062/14407459/644783d0-fe7e-11e5-9340-f6b06b44433f.jpg)\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/207897528", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-207897528", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 207897528, "node_id": "MDEyOklzc3VlQ29tbWVudDIwNzg5NzUyOA==", "user": {"login": "mikepb", "id": 637039, "node_id": "MDQ6VXNlcjYzNzAzOQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/637039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mikepb", "html_url": "https://github.com/mikepb", "followers_url": "https://api.github.com/users/mikepb/followers", "following_url": "https://api.github.com/users/mikepb/following{/other_user}", "gists_url": "https://api.github.com/users/mikepb/gists{/gist_id}", "starred_url": "https://api.github.com/users/mikepb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mikepb/subscriptions", "organizations_url": "https://api.github.com/users/mikepb/orgs", "repos_url": "https://api.github.com/users/mikepb/repos", "events_url": "https://api.github.com/users/mikepb/events{/privacy}", "received_events_url": "https://api.github.com/users/mikepb/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-10T02:07:39Z", "updated_at": "2016-04-10T02:07:39Z", "author_association": "NONE", "body": "Tricky :+1:\n\n> On Apr 9, 2016, at 5:54 PM, SJ notifications@github.com wrote:\n> \n> @henningpeters https://github.com/henningpeters I managed to compile and run pipe with OpenMP support. It looks beautiful when it utilizes all CPU cores. OpenMP support is by default disabled for MSVC. I added the compiler option to the setup file and committed the changes to the master branch. I'm surprised nobody is using Windows it seems. But with that switch enabled next releases will work properly on Windows\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub https://github.com/spacy-io/spaCy/issues/125#issuecomment-207892428\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/207941009", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-207941009", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 207941009, "node_id": "MDEyOklzc3VlQ29tbWVudDIwNzk0MTAwOQ==", "user": {"login": "henningpeters", "id": 166570, "node_id": "MDQ6VXNlcjE2NjU3MA==", "avatar_url": "https://avatars3.githubusercontent.com/u/166570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/henningpeters", "html_url": "https://github.com/henningpeters", "followers_url": "https://api.github.com/users/henningpeters/followers", "following_url": "https://api.github.com/users/henningpeters/following{/other_user}", "gists_url": "https://api.github.com/users/henningpeters/gists{/gist_id}", "starred_url": "https://api.github.com/users/henningpeters/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/henningpeters/subscriptions", "organizations_url": "https://api.github.com/users/henningpeters/orgs", "repos_url": "https://api.github.com/users/henningpeters/repos", "events_url": "https://api.github.com/users/henningpeters/events{/privacy}", "received_events_url": "https://api.github.com/users/henningpeters/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-10T07:49:51Z", "updated_at": "2016-04-10T07:49:51Z", "author_association": "CONTRIBUTOR", "body": "Oh, seems we overlooked msvc's openmp option. Thanks @sjjpo2002 for your PR! Just merged it.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/208743702", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-208743702", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 208743702, "node_id": "MDEyOklzc3VlQ29tbWVudDIwODc0MzcwMg==", "user": {"login": "henningpeters", "id": 166570, "node_id": "MDQ6VXNlcjE2NjU3MA==", "avatar_url": "https://avatars3.githubusercontent.com/u/166570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/henningpeters", "html_url": "https://github.com/henningpeters", "followers_url": "https://api.github.com/users/henningpeters/followers", "following_url": "https://api.github.com/users/henningpeters/following{/other_user}", "gists_url": "https://api.github.com/users/henningpeters/gists{/gist_id}", "starred_url": "https://api.github.com/users/henningpeters/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/henningpeters/subscriptions", "organizations_url": "https://api.github.com/users/henningpeters/orgs", "repos_url": "https://api.github.com/users/henningpeters/repos", "events_url": "https://api.github.com/users/henningpeters/events{/privacy}", "received_events_url": "https://api.github.com/users/henningpeters/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-12T07:10:19Z", "updated_at": "2016-04-12T08:01:03Z", "author_association": "CONTRIBUTOR", "body": "It seems the `/openmp` option is not available for some(?) Visual Studio express editions. That means many/most(?) Windows users won't be able to install spaCy anymore from source for free. There seems to exist a workaround (http://stackoverflow.com/questions/865686/openmp-in-visual-studio-2005-standard), but it's not something I'd like to put in our getting-started install guide. Hence, I think we should make OpenMP support on Windows optional for now.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/209036609", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-209036609", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 209036609, "node_id": "MDEyOklzc3VlQ29tbWVudDIwOTAzNjYwOQ==", "user": {"login": "sjjpo2002", "id": 8649062, "node_id": "MDQ6VXNlcjg2NDkwNjI=", "avatar_url": "https://avatars3.githubusercontent.com/u/8649062?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sjjpo2002", "html_url": "https://github.com/sjjpo2002", "followers_url": "https://api.github.com/users/sjjpo2002/followers", "following_url": "https://api.github.com/users/sjjpo2002/following{/other_user}", "gists_url": "https://api.github.com/users/sjjpo2002/gists{/gist_id}", "starred_url": "https://api.github.com/users/sjjpo2002/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sjjpo2002/subscriptions", "organizations_url": "https://api.github.com/users/sjjpo2002/orgs", "repos_url": "https://api.github.com/users/sjjpo2002/repos", "events_url": "https://api.github.com/users/sjjpo2002/events{/privacy}", "received_events_url": "https://api.github.com/users/sjjpo2002/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-12T18:10:06Z", "updated_at": "2016-04-12T18:10:06Z", "author_association": "CONTRIBUTOR", "body": "Microsoft Visual C++ Compiler for Python is [free ](https://www.microsoft.com/en-us/download/details.aspx?id=44266)for eveyone and it supports openmp. \n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/209038825", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-209038825", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 209038825, "node_id": "MDEyOklzc3VlQ29tbWVudDIwOTAzODgyNQ==", "user": {"login": "henningpeters", "id": 166570, "node_id": "MDQ6VXNlcjE2NjU3MA==", "avatar_url": "https://avatars3.githubusercontent.com/u/166570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/henningpeters", "html_url": "https://github.com/henningpeters", "followers_url": "https://api.github.com/users/henningpeters/followers", "following_url": "https://api.github.com/users/henningpeters/following{/other_user}", "gists_url": "https://api.github.com/users/henningpeters/gists{/gist_id}", "starred_url": "https://api.github.com/users/henningpeters/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/henningpeters/subscriptions", "organizations_url": "https://api.github.com/users/henningpeters/orgs", "repos_url": "https://api.github.com/users/henningpeters/repos", "events_url": "https://api.github.com/users/henningpeters/events{/privacy}", "received_events_url": "https://api.github.com/users/henningpeters/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-12T18:15:20Z", "updated_at": "2016-04-12T18:15:20Z", "author_association": "CONTRIBUTOR", "body": "Thanks, do you know whether this also works for Python 3.4/3.5? We need to have a solution in place that works as seamless as possible across all environments.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/209049764", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-209049764", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 209049764, "node_id": "MDEyOklzc3VlQ29tbWVudDIwOTA0OTc2NA==", "user": {"login": "sjjpo2002", "id": 8649062, "node_id": "MDQ6VXNlcjg2NDkwNjI=", "avatar_url": "https://avatars3.githubusercontent.com/u/8649062?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sjjpo2002", "html_url": "https://github.com/sjjpo2002", "followers_url": "https://api.github.com/users/sjjpo2002/followers", "following_url": "https://api.github.com/users/sjjpo2002/following{/other_user}", "gists_url": "https://api.github.com/users/sjjpo2002/gists{/gist_id}", "starred_url": "https://api.github.com/users/sjjpo2002/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sjjpo2002/subscriptions", "organizations_url": "https://api.github.com/users/sjjpo2002/orgs", "repos_url": "https://api.github.com/users/sjjpo2002/repos", "events_url": "https://api.github.com/users/sjjpo2002/events{/privacy}", "received_events_url": "https://api.github.com/users/sjjpo2002/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-12T18:46:35Z", "updated_at": "2016-04-12T18:46:35Z", "author_association": "CONTRIBUTOR", "body": "I never tried it on Python 3.x. But a quick check for /openmp [here](https://msdn.microsoft.com/en-us/library/fw509c3b%28v=vs.100%29.aspx) it is supported on VS 2010 ans 2015. For Python 3.4 using VS 2010 and for 3.5 VS 2015 are [suggested](http://stackoverflow.com/questions/29909330/microsoft-visual-c-compiler-for-python-3-4).\nBut, I agree that it should be tested on all platforms. Maybe I do it over the weekend. \n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/209056951", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-209056951", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 209056951, "node_id": "MDEyOklzc3VlQ29tbWVudDIwOTA1Njk1MQ==", "user": {"login": "henningpeters", "id": 166570, "node_id": "MDQ6VXNlcjE2NjU3MA==", "avatar_url": "https://avatars3.githubusercontent.com/u/166570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/henningpeters", "html_url": "https://github.com/henningpeters", "followers_url": "https://api.github.com/users/henningpeters/followers", "following_url": "https://api.github.com/users/henningpeters/following{/other_user}", "gists_url": "https://api.github.com/users/henningpeters/gists{/gist_id}", "starred_url": "https://api.github.com/users/henningpeters/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/henningpeters/subscriptions", "organizations_url": "https://api.github.com/users/henningpeters/orgs", "repos_url": "https://api.github.com/users/henningpeters/repos", "events_url": "https://api.github.com/users/henningpeters/events{/privacy}", "received_events_url": "https://api.github.com/users/henningpeters/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-12T19:07:37Z", "updated_at": "2016-04-12T19:07:37Z", "author_association": "CONTRIBUTOR", "body": "My reading about it so far is that MS only provides support for OpenMP in VS professional editions or higher. Here's the error output from VS 2010 express edition: https://ci.spacy.io/builders/spacy-win64-py34-64-install/builds/98/steps/shell_2/logs/stdio.\n\nIt might be possible to make it work as I suggested above, but so far I haven't seen a simple install instruction I would like to keep as default for spaCy all users.\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/224493856", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-224493856", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 224493856, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNDQ5Mzg1Ng==", "user": {"login": "aliabbasjp", "id": 830792, "node_id": "MDQ6VXNlcjgzMDc5Mg==", "avatar_url": "https://avatars0.githubusercontent.com/u/830792?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aliabbasjp", "html_url": "https://github.com/aliabbasjp", "followers_url": "https://api.github.com/users/aliabbasjp/followers", "following_url": "https://api.github.com/users/aliabbasjp/following{/other_user}", "gists_url": "https://api.github.com/users/aliabbasjp/gists{/gist_id}", "starred_url": "https://api.github.com/users/aliabbasjp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aliabbasjp/subscriptions", "organizations_url": "https://api.github.com/users/aliabbasjp/orgs", "repos_url": "https://api.github.com/users/aliabbasjp/repos", "events_url": "https://api.github.com/users/aliabbasjp/events{/privacy}", "received_events_url": "https://api.github.com/users/aliabbasjp/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-08T05:43:48Z", "updated_at": "2016-06-08T05:43:48Z", "author_association": "NONE", "body": "I think this is relevant:\n\nhttps://github.com/spacy-io/spaCy/issues/413\n"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/299717012", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-299717012", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 299717012, "node_id": "MDEyOklzc3VlQ29tbWVudDI5OTcxNzAxMg==", "user": {"login": "ines", "id": 13643239, "node_id": "MDQ6VXNlcjEzNjQzMjM5", "avatar_url": "https://avatars0.githubusercontent.com/u/13643239?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ines", "html_url": "https://github.com/ines", "followers_url": "https://api.github.com/users/ines/followers", "following_url": "https://api.github.com/users/ines/following{/other_user}", "gists_url": "https://api.github.com/users/ines/gists{/gist_id}", "starred_url": "https://api.github.com/users/ines/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ines/subscriptions", "organizations_url": "https://api.github.com/users/ines/orgs", "repos_url": "https://api.github.com/users/ines/repos", "events_url": "https://api.github.com/users/ines/events{/privacy}", "received_events_url": "https://api.github.com/users/ines/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-07T16:21:20Z", "updated_at": "2017-05-07T16:21:20Z", "author_association": "MEMBER", "body": "Closing this and making #1045 the master issue. Work in progress for spaCy v2.0!"}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/345434802", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-345434802", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 345434802, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NTQzNDgwMg==", "user": {"login": "easimadi", "id": 13008169, "node_id": "MDQ6VXNlcjEzMDA4MTY5", "avatar_url": "https://avatars3.githubusercontent.com/u/13008169?v=4", "gravatar_id": "", "url": "https://api.github.com/users/easimadi", "html_url": "https://github.com/easimadi", "followers_url": "https://api.github.com/users/easimadi/followers", "following_url": "https://api.github.com/users/easimadi/following{/other_user}", "gists_url": "https://api.github.com/users/easimadi/gists{/gist_id}", "starred_url": "https://api.github.com/users/easimadi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/easimadi/subscriptions", "organizations_url": "https://api.github.com/users/easimadi/orgs", "repos_url": "https://api.github.com/users/easimadi/repos", "events_url": "https://api.github.com/users/easimadi/events{/privacy}", "received_events_url": "https://api.github.com/users/easimadi/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-18T11:02:03Z", "updated_at": "2017-11-18T11:02:03Z", "author_association": "NONE", "body": "Dears,\r\nIs this issue resolved with the release of spacy 2.0.\r\nHow can I use spacy in Spark?\r\n\r\nThanks for your help.\r\nea."}, {"url": "https://api.github.com/repos/explosion/spaCy/issues/comments/387303801", "html_url": "https://github.com/explosion/spaCy/issues/125#issuecomment-387303801", "issue_url": "https://api.github.com/repos/explosion/spaCy/issues/125", "id": 387303801, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NzMwMzgwMQ==", "user": {"login": "lock[bot]", "id": 33595554, "node_id": "MDM6Qm90MzM1OTU1NTQ=", "avatar_url": "https://avatars1.githubusercontent.com/in/6672?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lock%5Bbot%5D", "html_url": "https://github.com/apps/lock", "followers_url": "https://api.github.com/users/lock%5Bbot%5D/followers", "following_url": "https://api.github.com/users/lock%5Bbot%5D/following{/other_user}", "gists_url": "https://api.github.com/users/lock%5Bbot%5D/gists{/gist_id}", "starred_url": "https://api.github.com/users/lock%5Bbot%5D/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lock%5Bbot%5D/subscriptions", "organizations_url": "https://api.github.com/users/lock%5Bbot%5D/orgs", "repos_url": "https://api.github.com/users/lock%5Bbot%5D/repos", "events_url": "https://api.github.com/users/lock%5Bbot%5D/events{/privacy}", "received_events_url": "https://api.github.com/users/lock%5Bbot%5D/received_events", "type": "Bot", "site_admin": false}, "created_at": "2018-05-08T06:55:49Z", "updated_at": "2018-05-08T06:55:49Z", "author_association": "NONE", "body": "This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.\n"}]